{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ef38024-d513-436b-9b95-7996a4ec69c1",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "### Simplified System of linear equations solving for image $X$\n",
    "\n",
    "Key terms:\n",
    "\n",
    "$L$ = one-dimensional size for high-resolution image\n",
    "\n",
    "$M$ = one-dimensional size for low-resolution image\n",
    "\n",
    "$X$ = $L^2 \\times 1$ column vector, represents the ideal image we are trying to recover via solving\n",
    "\n",
    "$Y_i$ = $M^2 \\times 1$ column vector, represents a decimated, downsampled, blurred, noisy image based on $X$\n",
    "\n",
    "$N$ = the number of low-resolution images\n",
    "\n",
    "$D$ = the decimation matrix operator of size $M^2 \\times L^2$\n",
    "\n",
    "$H$ = the blur matrix operator of size $L^2 \\times L^2$\n",
    "\n",
    "$S$ = the sharpening operator of size $L^2 \\times L^2$, i.e., the laplacian operator\n",
    "\n",
    "$w_i$ = a weighting scalar, a \"confidence factor\" for each $Y_i$ (eventually a diagonal matrix?)\n",
    "\n",
    "$\\beta$ = smoothing factor for controlling image sharpening\n",
    "\n",
    "---\n",
    "\n",
    "Objective: to solve for $X$ (i.e., $AX = B$) based on the following equation:\n",
    "\n",
    "$\\begin{bmatrix}\n",
    "  \\beta S^T S + (\\sum\\limits_{i=1}^N w_i) H^TD^TDH\n",
    "\\end{bmatrix}X = \\sum\\limits_{i=1}^N (w_iH^TD^TY_i)$\n",
    "\n",
    "Note: In order to reconstruct a high-resolution image from $N$ low-resolution images, the following criteria must be satisfied: $L^2 \\leq \\min \\left\\{ \\left(s^2 - 2 \\right)M^2, NM^2\\right\\}$, where $s$ is the full support of the blurring kernel (i.e., point-spread function) (as per section III.B of [1]).\n",
    "\n",
    "[1] M. Elad and A. Feuer, \"Restoration of a single superresolution image from several blurred, noisy, and undersampled measured images,\" in IEEE Transactions on Image Processing, vol. 6, no. 12, pp. 1646-1658, Dec. 1997, doi: 10.1109/83.650118.\n",
    "\n",
    "---\n",
    "\n",
    "General information:\n",
    "\n",
    "- Dataset: GLEAM Small\n",
    "- Timesteps: 30\n",
    "- Receivers: 512\n",
    "- Channels: 1\n",
    "- $L$ = 100 pixels\n",
    "- $M$ = 50 pixels\n",
    "- $N$ = 6 images (30 timesteps / 5 timesteps per image)\n",
    "- $w_i$ = 1 for all images\n",
    "- $\\beta$ = 1 (I guess?)\n",
    "\n",
    "Assumptions:\n",
    "1. The blur kernel $H$ is uniform across all instances of $Y$\n",
    "2. The decimatimation kernel $D$ is uniform across all instances of $Y$\n",
    "3. The $w_i$ is uniform across all instances of $Y$, just a confidence of 1.0 (\"full confidence\")\n",
    "\n",
    "---\n",
    "\n",
    "#### To do:\n",
    "\n",
    "- [x] Generate 6 dataset subsets, containing timesteps each (i.e., 30 time steps / 5 time steps per file)\n",
    "- [x] Generate one point spread function per dataset subset, for analysis of different\n",
    "- [x] Generate one point spread function for the whole dataset\n",
    "- [x] Determine if there is a significant difference between a subset PSF and the full dataset PSF - this will be the blur kernel\n",
    "- [ ] Determine if the PSF can be reduced (i.e., dont use the full PSF) - what effects might this have?\n",
    "- [x] Generate an IDFT of the full dataset, $L^2$ pixels\n",
    "- [x] Genetate an IDFT for each of the subsets, $M^2$ pixels\n",
    "- [x] Come up with a formula for populating the decimation matrix\n",
    "- [ ] Come up with a formula for populating the blur matrix \n",
    "- [ ] Determine if there is a formula for populating the product of the blur and decimation matrix, such that $H^TD^T = (HD)^T$\n",
    "- [ ] take the average of optimal betas across support 20 to 47, use that for all supports, plot the error between solved x and true x using the averaged beta, and compare that against another plot which is the \"optimal\" beta vs support\n",
    "- [ ] another thing to try, consider trimming the edge of solved x before taking norms and measuring RRMSE, as the convolution matrices are not padded and so convolution is essentially \"incomplete\" around the image edges. Probably only need to take say 10-15 pixels off all edges, since most of the PSF is noise aside from the center.\n",
    "- [ ] Another thing to try is to produce a set of Y images which are only representative of a single source, like a PSF. Then I could run the SOLE algorithm to determine what the effects of solving are against the curve of a point source, by taking a middle slice of the image (like I do when inspecting the actual PSF). Could examine what happens when using different supports and beta terms to see which lines up best with the true curve.\n",
    " - [ ] I need to modify the decimation matrix function to support more than just 4 neighbours (or 1/2 decimation).\n",
    " - [ ] Modify the project so it is more like a library, where I can call my own functions in my desired order. This would make repeating experiments easier than one flat notebook.\n",
    " - [ ] Implement the laplacian of gaussian function, so that I can have a dynamically sized laplacian instead of a fixed 3x3 (note: requires another config to specify laplacian support size, and a sigma value to determine curve spread, or are they roughly the same type of thing?)\n",
    " - [ ] Try running the algorithm with the averaged beta, using different sized LoGs and different PSF supports to see what happens re: rrmse\n",
    " - [ ] Figure out how I can get the sigma using a point of inflection of a large PSF (something like that?)\n",
    " - [ ] Implement assertion to confirm number of y samples and decimation factor follow critical formula from adaptive filtering approach paper\n",
    " - [ ] June 9 - Andrew suggests updating my decimation routine to be more dynamic, supporting powers of 2 or even arbitrary amounts, not sure how hard that is? He suggests experimenting with variable decimation and using a fixed optimal beta, fixed optimal sigma, and fixed PSF support (one of the higher end supports which seem stable, say 51? Could probably even just do the full PSF of 99x99 realistically\n",
    "\n",
    "---\n",
    "\n",
    "#### Notes:\n",
    "\n",
    "- If one takes the $\\beta S^T S$ portion of $A$ and multiplies it by the solved $X$;  assuming the result is not normalised, one can observe the contribution of beta-influenced laplacian sharpening towards $B$. It appears to float around 10% of the second portion of $A$ (the decimated PSF) multiplied by the solved $X$. Taking the sum of these two separate portions of $AX$ allow one to produce $B$. It appears that the laplacian sharpening portion of $A$ does meaningfully contribute to help find a reasonable solution to $X$, and doesnt appear to be dominating (i.e., drowning out) the decimated PSF portion of $A$.\n",
    "- Essentially, we dont want the beta-driven laplacian to dominate the sharpened PSF, otherwise you end up with effectively an overly smooth image $X$, and the technique becomes less useful / less impressive. We want to solve for $X$, such that beta helps us resolve a more accurate image $X$ which isnt just noise (a lack of structure, looks like static), and also doesnt look too smooth. If solving was less useful because we required large beta terms, then we might as well just average our set of $Y$ low-resolution images instead since its computationally cheaper?\n",
    " - Implement nifty gridder as a jupyter notebook - Andrew wants to see an image of true x, solved x, and nifty x\n",
    " - Andrew suggestions that in the future, a good method for analysing the effectiveness of this technique would be to compare the extractable sources and compare against a sky model (possibly Fred Dulwich has this?). It seems RRMSE is \"misleading\", in the sense that images produced by this technique arent so noisy, sources appear more gaussian-like and less like the dirty beam.\n",
    " - Furthermore, I should have a look at how this technique compares against the direct image model, versus an image produced by means of gridding (i.e., w-projection, nifty).\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05a6d0bd-b71e-40c2-a6a1-a02d42954095",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext line_profiler\n",
    "%load_ext memory_profiler\n",
    "\n",
    "from linear_system_super_resolution import * # not best practice, but oh well\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import cupy as cp\n",
    "from cupyx.scipy.sparse.linalg import cg as cg_gpu\n",
    "from cupyx.scipy.sparse.linalg import aslinearoperator as aslinearoperator_gpu\n",
    "from cupyx.scipy.sparse import csr_matrix as csr_matrix_gpu\n",
    "# from cupyx.scipy.sparse.linalg import lsqr as cuda_sparse_solve\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.signal import convolve2d\n",
    "from scipy.stats import norm\n",
    "from scipy.sparse import csr_matrix, bsr_matrix, dia_matrix, diags, csr_array\n",
    "from scipy.sparse.linalg import bicg, bicgstab, cg, cgs, gmres, lgmres, minres, qmr, gcrotmk, tfqmr, lsqr, lsmr, aslinearoperator\n",
    "from scipy.linalg import solve\n",
    "from skimage import data, io, color\n",
    "from skimage.transform import resize\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "# from sparse_dot_mkl import dot_product_mkl as parallel_mult\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [15, 15]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a889ad-d239-41b9-9507-a98d1effc956",
   "metadata": {},
   "source": [
    "### Notes from Andrew - 14 Feb\n",
    "- There seems to be a misalignment through the decimation/upsampling process - consider manually aligning the solution x and true x to see if once aligned the RRMSE is signficiantly lower...\n",
    "- Furthermore, there might need to be some form of 'weighting' applied during decimation/upsampling to better approximation pixels when downsampling/upsampling instead of hard averages of pixels - gaussian weighting over sampling region?\n",
    "- Andrew suggests playing around with different solvers to see what effects they have on imaging\n",
    "- Furthermore, try using increased iteration counts to see how the solution x fits to true x in terms of line plotting\n",
    "- Another important thing to play with is to try and fit the laplacian of gaussian to the point spread function, ideally over the main lobe. Consider manually fitting the LoG to the PSF in the LoG notebook, ideally have the same number of samples for both, and play with sigma to fit it to the PSF\n",
    "- Notable finding is that it seems the peak of the source in the single source approach is smeared over 4 pixels. I wonder if I use a different image scaling like 270 down to 80 or something like that, where its a downsample factor of 3, to see if that makes the source peak spread over 3x3 pixels or something like that..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b98346-6318-477d-8555-44909edbde9c",
   "metadata": {},
   "source": [
    "#### Configuration and data set up..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7561452-d065-4b75-903d-3830a23770d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps = 30 # total timesteps\n",
    "timesteps_per_y = 1\n",
    "l = 300\n",
    "m = 100\n",
    "n = timesteps // timesteps_per_y\n",
    "downsample = l // m\n",
    "\n",
    "beta = 1.8 # 1.8\n",
    "blur_kernel_support = 299\n",
    "# Below should be some factor of m**2\n",
    "dh_matrix_batch_size = m**2\n",
    "num_solver_iterations = 30\n",
    "\n",
    "# Note: flowchart for selecting solver routine might be useful..\n",
    "# https://www.mathworks.com/help/matlab/math/iterative-methods-for-linear-systems.html\n",
    "solver = tfqmr\n",
    "\n",
    "# False = use static 3x3 laplacian kernel\n",
    "# True = use customisable laplacian of gaussian kernel\n",
    "use_LoG = False\n",
    "\n",
    "# False = solve using CPU based scipy library\n",
    "# True = solve using GPU based cupy library\n",
    "accelerated = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6aaabe-fc4b-40d8-b4d7-7091013e7151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine whether we have the right criteria to perform super-resolution imaging to obtain X\n",
    "if l**2 <= np.minimum((blur_kernel_support - 2) * m**2, n * m**2):\n",
    "    pass #print(\"Super-resolution imaging technically possible, continuing...\\n\")\n",
    "else:\n",
    "    raise ValueError(\"High-resolution image not possible to reproduce from your set up, review your configuration against this if statement conditional.\\n\") \n",
    "    \n",
    "# Image weights, assumed 1.0 for all images (full confidence)\n",
    "weights = np.ones(n)\n",
    "    \n",
    "# all time steps direct image, the reference image    \n",
    "x_true = load_image(f\"../datasets/gleam_small/images/single_source/direct_image_ts_0_29_{l}x{l}.bin\", l, True)\n",
    "# all time steps direct image, the blur kernel for H matrix\n",
    "x_psf = load_image(f\"../datasets/gleam_small/images/single_source/direct_psf_ts_0_29_{l}x{l}.bin\", l)\n",
    "blur_kernel_support_half = (blur_kernel_support - 1) // 2\n",
    "trim_min = l//2 - (blur_kernel_support_half)\n",
    "trim_max = l//2 + blur_kernel_support_half + 1\n",
    "x_psf_trimmed = x_psf.copy()[trim_min : trim_max, trim_min : trim_max]\n",
    "x_psf_trimmed /= np.sum(x_psf_trimmed)\n",
    "\n",
    "# Storing all low-res images as layered stack\n",
    "y = np.zeros((n, m, m))\n",
    "\n",
    "# batched time steps direct images\n",
    "for i in range(n):\n",
    "    timestep_range_start = i * timesteps_per_y\n",
    "    timestep_range_end = timestep_range_start + timesteps_per_y\n",
    "    y[i] = load_image(f\"../datasets/gleam_small/images/single_source/direct_image_ts_{timestep_range_start}_{timestep_range_end - 1}_{m}x{m}.bin\", m, True)\n",
    "\n",
    "# Sharpening matrix S\n",
    "s_mat = generate_s_matrix(l)\n",
    "# if use_LoG:\n",
    "#     β *= 4.0\n",
    "\n",
    "# Matrix product of D (decimation) and H (blur) matrices\n",
    "dh_mat = generate_dh_matrix_batched(x_psf_trimmed, l, downsample, dh_matrix_batch_size)\n",
    "\n",
    "# High resolution estimate of X\n",
    "b_mat = generate_b_image(l, n, weights, dh_mat, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8484fed-ac14-4408-b862-7d92b24589bf",
   "metadata": {},
   "source": [
    "#### Use below for calling one-off executions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca8ccfc-9d6b-428d-817f-830243629802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Super-resolution imaging technique now callable via func (specifically so I can perform bulk testing on different beta and sigma values)\n",
    "# linear_system_imager(l, m, n, downsample, timesteps, timesteps_per_y, beta, blur_kernel_support, dh_matrix_batch_size, num_solver_iterations, solver, use_LoG, accelerated)\n",
    "# %lprun -f linear_system_imager linear_system_imager(l, m, n, downsample, timesteps, timesteps_per_y, beta, blur_kernel_support, dh_matrix_batch_size, num_solver_iterations, solver, use_LoG, accelerated)\n",
    "\n",
    "# s_mat = generate_s_matrix(l, True, 151, 10.0)\n",
    "x_solution = linear_system_imager_solve(s_mat, dh_mat, b_mat, x_true, l, beta, weights, solver, num_solver_iterations, rrmse_per_beta_per_solve_iter=None, current_beta_and_iteration_indices=None)\n",
    "show_image(x_solution, \"Final X Solution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa8d5f7-cc1b-4087-865f-5477eec78777",
   "metadata": {},
   "source": [
    "#### Use below for batched beta/solver testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd012de6-46aa-42f0-9442-06bc580b4c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# solvers = [tfqmr]\n",
    "\n",
    "# for sol in solvers:\n",
    "#     solver = sol\n",
    "#     print(f\"--- Working on solver {solver.__name__}\")\n",
    "#     betas = np.linspace(0.5, 20.0, 10)\n",
    "#     rrmse_per_beta_per_solve_iter = np.zeros((betas.shape[0], num_solver_iterations))\n",
    "#     current_beta_and_iteration_indices = np.zeros(2, dtype=np.uintc) # simulates a pass by reference counter since passing vals to each solver iteration is clunky...\n",
    "\n",
    "#     for b in range(betas.shape[0]):\n",
    "#         current_beta_and_iteration_indices[0] = b\n",
    "#         beta = betas[b]\n",
    "#         print(f\"Batch {b} working on beta {beta}\")\n",
    "#         # linear_system_imager(l, m, n, downsample, timesteps, timesteps_per_y, beta, blur_kernel_support, dh_matrix_batch_size, num_solver_iterations, solver, use_LoG, accelerated, rrmse_per_beta_per_solve_iter, current_beta_and_iteration_indices)\n",
    "#         linear_system_imager_solve(s_mat, dh_mat, b_mat, x_true, l, beta, weights, solver, num_solver_iterations, rrmse_per_beta_per_solve_iter, current_beta_and_iteration_indices)\n",
    "        \n",
    "#     df = pd.DataFrame(rrmse_per_beta_per_solve_iter.T)\n",
    "#     df.columns = betas\n",
    "#     df.to_csv(f\"rrmse_per_beta_per_solve_iter - {solver.__name__}.csv\")\n",
    "#     df.plot.line()\n",
    "#     print()\n",
    "    \n",
    "# print(\"Finished...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25ca5d5-eaf4-4117-a25a-1551a9ff7d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_columns', None)\n",
    "# df = pd.DataFrame(rrmse_per_beta_per_solve_iter.T)\n",
    "# df.columns = betas\n",
    "\n",
    "# # df.to_csv(\"rrmse_per_beta_per_solve_iter - TFQMR.csv\")\n",
    "\n",
    "# display(df)\n",
    "\n",
    "# df.plot.line()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e516371a-2cbe-46d9-be15-b0c9f4454532",
   "metadata": {},
   "source": [
    "#### Use below for batched laplacian of gaussian testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8602b998-8fbf-4b80-88e2-4fb009af6b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# beta = 1.8\n",
    "# sigmas = np.linspace(500.0, 2000.0, 10)\n",
    "# log_size = 199\n",
    "# rrmse_per_sigma_per_solve_iter = np.zeros((sigmas.shape[0], num_solver_iterations))\n",
    "# current_sigma_and_iteration_indices = np.zeros(2, dtype=np.uintc) # simulates a pass by reference counter since passing vals to each solver iteration is clunky...\n",
    "\n",
    "# for s in range(sigmas.shape[0]):\n",
    "#     current_sigma_and_iteration_indices[0] = s\n",
    "#     sigma = sigmas[s]\n",
    "#     print(f\"Batch {s} working on sigma {sigma}\")\n",
    "#     s_mat = generate_s_matrix(l, True, log_size, sigma)\n",
    "#     # linear_system_imager(l, m, n, downsample, timesteps, timesteps_per_y, beta, blur_kernel_support, dh_matrix_batch_size, num_solver_iterations, solver, use_LoG, accelerated, rrmse_per_beta_per_solve_iter, current_beta_and_iteration_indices)\n",
    "#     x_solution = linear_system_imager_solve(s_mat, dh_mat, b_mat, x_true, l, beta, weights, solver, num_solver_iterations, rrmse_per_sigma_per_solve_iter, current_sigma_and_iteration_indices)\n",
    "#     show_image(x_solution, \"X Sol\")\n",
    "    \n",
    "# df = pd.DataFrame(rrmse_per_sigma_per_solve_iter.T)\n",
    "# df.columns = sigmas\n",
    "# # df.to_csv(\"rrmse_per_sigma_per_solve_iter - TFQMR.csv\")\n",
    "# display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f072b064-89f3-4e77-9b4b-43c45b220ef0",
   "metadata": {},
   "source": [
    "#### Below is the source code before it was merged into the linear_system_imager func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1af324-29fa-4715-885e-295495127fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # all time steps direct image\n",
    "# filename = f\"../datasets/gleam_small/images/single_source/direct_image_ts_0_29_{l}x{l}.bin\"\n",
    "# x_true = normalise(np.fromfile(filename, dtype=np.float32).reshape(l, l))\n",
    "# # show_image(x_true, \"True X\")\n",
    "\n",
    "# filename = f\"../datasets/gleam_small/images/single_source/direct_psf_ts_0_29_{l}x{l}.bin\"\n",
    "# x_psf = np.fromfile(filename, dtype=np.float32).reshape(l, l)\n",
    "# # show_image(x_psf, \"Untrimmed PSF\")\n",
    "# plt.plot(x_psf[x_psf.shape[0]//2])\n",
    "# plt.show()\n",
    "\n",
    "# blur_kernel_support_half = (blur_kernel_support - 1) // 2\n",
    "# trim_min = l//2 - (blur_kernel_support_half)\n",
    "# trim_max = l//2 + blur_kernel_support_half + 1\n",
    "# print(f\"PSF min/max/support: {trim_min}, {trim_max}, {blur_kernel_support}\")\n",
    "# # x_psf = x_psf.reshape(l, l)[psf_min:psf_max, psf_min:psf_max]\n",
    "# x_psf_trimmed = x_psf.copy()[trim_min : trim_max, trim_min : trim_max]\n",
    "# x_psf_trimmed /= np.sum(x_psf_trimmed)\n",
    "# # print(np.sum(x_psf_trimmed))\n",
    "# plt.plot(x_psf_trimmed[x_psf_trimmed.shape[0]//2])\n",
    "# plt.show()\n",
    "\n",
    "# # print(f\"PSF Shape: {x_psf_trimmed.shape[0]}\")\n",
    "# # show_image(x_psf_trimmed, \"PSF\")\n",
    "\n",
    "# print(f\"l^2:                      {l**2}\")\n",
    "# print(f\"(psf_support - 2) * m^2:  {(blur_kernel_support - 2) * m**2}\")\n",
    "# print(f\"n * m^2:                  {n * m**2}\")\n",
    "\n",
    "# # Determine whether we have the right criteria to perform super-resolution imaging to obtain X\n",
    "# if l**2 <= np.minimum((blur_kernel_support - 2) * m**2, n * m**2):\n",
    "#     print(\"Super-resolution imaging technically possible, continuing...\\n\")\n",
    "# else:\n",
    "#     raise ValueError(\"High-resolution image not possible to reproduce from your set up, review your configuration against this if statement conditional.\\n\")\n",
    "\n",
    "# # Storing all low-res images as layered stack\n",
    "# y = np.zeros((n, m, m))\n",
    "\n",
    "# # batched time steps direct images\n",
    "# for i in np.arange(n):\n",
    "#     timestep_range_start = i * timesteps_per_y\n",
    "#     timestep_range_end = timestep_range_start + timesteps_per_y\n",
    "#     filename = f\"../datasets/gleam_small/images/single_source/direct_image_ts_{timestep_range_start}_{timestep_range_end - 1}_{m}x{m}.bin\"\n",
    "#     y[i] = np.fromfile(filename, dtype=np.float32).reshape(m, m)\n",
    "#     y[i] = normalise(y[i])\n",
    "\n",
    "# s = generate_s_matrix(l, use_laplacian_of_gaussian, 51)\n",
    "# print(matrix_memory(s))\n",
    "\n",
    "# # if use_laplacian_of_gaussian:\n",
    "# #     β *= 4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd6a15f-0815-4697-a031-391d2eba1650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %time h = generate_h_matrix(l, x_psf_trimmed)\n",
    "\n",
    "# # Decimation matrix\n",
    "# # d = generate_d_matrix(l, 4)\n",
    "\n",
    "# # show_sparse_matrix(d, \"D\")\n",
    "# # d_numpy_size_bytes = d.size * d.itemsize\n",
    "# # print(f\"D matrix uncompressed sparse: {d_numpy_size_bytes / 10**6} MB\")\n",
    "# # d = bsr_matrix(d)\n",
    "# # print(f\"D matrix memory usage: {matrix_memory(d)}MB\")\n",
    "# # print(f\"D matrix density: {matrix_density(d)}%\")\n",
    "# # print(f\"D matrix reduced by {100 - (d_csr_matrix_bytes / d_numpy_size_bytes * 100)}%\\n\")\n",
    "\n",
    "# # dh = parallel_mult(d, h)\n",
    "# # %time dh = d @ h\n",
    "# # show_sparse_matrix(dh, \"DH\")\n",
    "\n",
    "# # print(f\"DH matrix memory usage: {matrix_memory(dh)}MB\")\n",
    "# # print(f\"DH matrix density: {matrix_density(dh)}%\")\n",
    "\n",
    "# # %lprun -f generate_dh_matrix_batched dh = generate_dh_matrix_batched(x_psf_trimmed, l, downsample, dh_matrix_batch_size)\n",
    "# dh = generate_dh_matrix_batched(x_psf_trimmed, l, downsample, dh_matrix_batch_size)\n",
    "\n",
    "# # show_sparse_matrix(dh_numba, \"DH Numba\")\n",
    "# print(f\"DH matrix memory usage: {matrix_memory(dh)}MB\")\n",
    "    \n",
    "# # Notes to self: seems like lap_gauss_σ sits around 1.41 for this example, any higher or lower results in worse RRMSE. Changing the psf_central_support makes no difference, and taking more samples (i.e. num)\n",
    "# # just means that lap_gauss_σ is multiplied by the factor of difference (i.e., double the num samples needs double the lap_gauss_σ). Also found that beta is best multiplied by 4.0, not 16 (the square).\n",
    "# # Not sure what to do from here... seems like the laplacian of gaussian doesnt really serve any beneficial purpose..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f5877a-f42d-4bbe-9815-09fba1baae3f",
   "metadata": {},
   "source": [
    "#### Setting up the right hand side of the equation $AX = B$, where $B = \\sum\\limits_{i=1}^N (w_iH^TD^TY_i)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70b5b39-97ff-454d-a18c-be18ca528ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %time b = generate_b_image(l, n, w, dh, y)\n",
    "# # show_image(b.reshape(l, l), \"B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42023895-557c-4c97-a10e-bf1a2b43d09a",
   "metadata": {},
   "source": [
    "#### Setting up the right hand side of the equation $AX=B$, where $A = \\begin{bmatrix}\n",
    "  \\beta S^T S + (\\sum\\limits_{i=1}^N w_i) H^TD^TDH\n",
    "\\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cad2eee-2322-4931-825f-0efa8ca8c419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %time lhs = (β * (s @ csr_matrix.transpose(s)))\n",
    "# print(f\"A_LHS l2 norm: {np.sum(lhs**2)}\")\n",
    "# print(f\"A_LHS matrix memory usage: {matrix_memory(lhs)}MB\")\n",
    "# print(f\"A_LHS matrix density: {matrix_density(lhs)}%\")\n",
    "\n",
    "# # This section is painfully slow... like 3-4 mins\n",
    "# %time rhs = ((csr_matrix.transpose(dh) @ dh) * np.sum(w))\n",
    "# print(f\"A_RHS matrix memory usage: {matrix_memory(rhs)}MB\")\n",
    "# print(f\"A_RHS matrix density: {matrix_density(rhs)}%\")\n",
    "\n",
    "# %time a = lhs + rhs\n",
    "# print(f\"A matrix memory usage: {matrix_memory(a)}MB\")\n",
    "# print(f\"A matrix density: {matrix_density(a)}%\")\n",
    "\n",
    "# runtime for scipy mat mul = 4 min 26 sec\n",
    "# runtime for parallel mult = \n",
    "\n",
    "# a = bsr_matrix.toarray(a)\n",
    "# show_image(a.reshape(l**2, l**2), \"A\")\n",
    "# print(f\"A l2 norm: {np.linalg.norm(a)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f646df-d788-465a-b6b6-f0a308d22e9a",
   "metadata": {},
   "source": [
    "#### Now solve for $X$..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c87506-72dd-42e6-8e3e-c83c821d9a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use_accelerated_solving = False\n",
    "\n",
    "# if use_accelerated_solving:\n",
    "#     # CUPY WORK START\n",
    "#     s_gpu = csr_matrix_gpu(s)\n",
    "#     dh_gpu = csr_matrix_gpu(dh)\n",
    "#     slo_gpu = aslinearoperator_gpu(s_gpu) # does s need to be put onto gpu?\n",
    "#     dhlo_gpu = aslinearoperator_gpu(dh_gpu) # does dh need to be put onto gpu?\n",
    "#     b_gpu = cp.array(b)\n",
    "#     w_sum = np.sum(w)\n",
    "#     alo_gpu = (β * (slo_gpu.T @ slo_gpu)) + ((dhlo_gpu.T @ dhlo_gpu) * w_sum)\n",
    "#     xlo_gpu, info = cg_gpu(alo_gpu, b_gpu, maxiter=100)\n",
    "#     # print(f\"GPU INFO: {info}\")\n",
    "#     xlo = cp.asnumpy(xlo_gpu).reshape(l, l)\n",
    "#     # CUPY WORK END\n",
    "# else:\n",
    "    \n",
    "#     # Callback for iterative solution\n",
    "#     cb = lambda current_solution : show_iter_solution(current_solution, x_true)\n",
    "    \n",
    "#     slo = aslinearoperator(s)\n",
    "#     dhlo = aslinearoperator(dh)\n",
    "#     alo = (β * (slo.T @ slo)) + ((dhlo.T @ dhlo) * np.sum(w))\n",
    "#     xlo, info = solver(alo, b, maxiter=30, callback=cb) #, callback_type='x')\n",
    "#     xlo = xlo.reshape(l, l)\n",
    "\n",
    "# # x, _ = solver(a, b, maxiter = 1)\n",
    "# # x = x.reshape(l, l)\n",
    "# # show_image(normalise(x), \"Solved X\", flip_x_axis=True)\n",
    "# # print(f\"Beta: {β}\")\n",
    "# # print(f\"RRMSE: Solved X and True X -> {rrmse(normalise(x), normalise(x_true))}\")\n",
    "# # print(f\"RRMSE: Solved X and B -> {rrmse(normalise(x), normalise(b.reshape(l, l)))}\")\n",
    "# # print(f\"True X peak location: {np.unravel_index(np.argmax(x_true, axis=None), x_true.shape)}\")\n",
    "# # print(f\"Solved X peak location: {np.unravel_index(np.argmax(x, axis=None), x.shape)}\")\n",
    "\n",
    "# # show_image(normalise(xlo), \"Solved X (Linear Operator)\", flip_x_axis=True)\n",
    "# # show_image(normalise(x_true), \"True X\", flip_x_axis=True)\n",
    "\n",
    "# print(f\"Beta: {β}\")\n",
    "# print(f\"RRMSE: Solved X and True X -> {rrmse(normalise(xlo), normalise(x_true))}\")\n",
    "# print(f\"RRMSE: Solved X and B -> {rrmse(normalise(xlo), normalise(b.reshape(l, l)))}\")\n",
    "# print(f\"RRMSE: B and True X -> {rrmse(normalise(b.reshape(l, l)), normalise(x_true))}\")\n",
    "# print(f\"True X peak location: {np.unravel_index(np.argmax(x_true, axis=None), x_true.shape)}\")\n",
    "# print(f\"Solved X peak location: {np.unravel_index(np.argmax(xlo, axis=None), xlo.shape)}\")\n",
    "\n",
    "# # plt.plot(normalise(xlo[202, :]))\n",
    "# # plt.plot(normalise(x_true[200, :]))\n",
    "# # plt.show()\n",
    "\n",
    "# # print(rrmse(normalise(x), normalise(xlo)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

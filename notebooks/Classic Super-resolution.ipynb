{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ef38024-d513-436b-9b95-7996a4ec69c1",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "### Simplified System of linear equations solving for image $X$\n",
    "\n",
    "Key terms:\n",
    "\n",
    "$L$ = one-dimensional size for high-resolution image\n",
    "\n",
    "$M$ = one-dimensional size for low-resolution image\n",
    "\n",
    "$X$ = $L^2 \\times 1$ column vector, represents the ideal image we are trying to recover via solving\n",
    "\n",
    "$Y_i$ = $M^2 \\times 1$ column vector, represents a decimated, downsampled, blurred, noisy image based on $X$\n",
    "\n",
    "$N$ = the number of low-resolution images\n",
    "\n",
    "$D$ = the decimation matrix operator of size $M^2 \\times L^2$\n",
    "\n",
    "$H$ = the blur matrix operator of size $L^2 \\times L^2$\n",
    "\n",
    "$S$ = the sharpening operator of size $L^2 \\times L^2$, i.e., the laplacian operator\n",
    "\n",
    "$w_i$ = a weighting scalar, a \"confidence factor\" for each $Y_i$ (eventually a diagonal matrix?)\n",
    "\n",
    "$\\beta$ = smoothing factor for controlling image sharpening\n",
    "\n",
    "---\n",
    "\n",
    "Objective: to solve for $X$ (i.e., $AX = B$) based on the following equation:\n",
    "\n",
    "$\\begin{bmatrix}\n",
    "  \\beta S^T S + (\\sum\\limits_{i=1}^N w_i) H^TD^TDH\n",
    "\\end{bmatrix}X = \\sum\\limits_{i=1}^N (w_iH^TD^TY_i)$\n",
    "\n",
    "Note: In order to reconstruct a high-resolution image from $N$ low-resolution images, the following criteria must be satisfied: $L^2 \\leq \\min \\left\\{ \\left(s^2 - 2 \\right)M^2, NM^2\\right\\}$, where $s$ is the full support of the blurring kernel (i.e., point-spread function) (as per section III.B of [1]).\n",
    "\n",
    "[1] M. Elad and A. Feuer, \"Restoration of a single superresolution image from several blurred, noisy, and undersampled measured images,\" in IEEE Transactions on Image Processing, vol. 6, no. 12, pp. 1646-1658, Dec. 1997, doi: 10.1109/83.650118.\n",
    "\n",
    "---\n",
    "\n",
    "General information:\n",
    "\n",
    "- Dataset: GLEAM Small\n",
    "- Timesteps: 30\n",
    "- Receivers: 512\n",
    "- Channels: 1\n",
    "- $L$ = 100 pixels\n",
    "- $M$ = 50 pixels\n",
    "- $N$ = 6 images (30 timesteps / 5 timesteps per image)\n",
    "- $w_i$ = 1 for all images\n",
    "- $\\beta$ = 1 (I guess?)\n",
    "\n",
    "Assumptions:\n",
    "1. The blur kernel $H$ is uniform across all instances of $Y$\n",
    "2. The decimatimation kernel $D$ is uniform across all instances of $Y$\n",
    "3. The $w_i$ is uniform across all instances of $Y$, just a confidence of 1.0 (\"full confidence\")\n",
    "\n",
    "---\n",
    "\n",
    "#### To do:\n",
    "\n",
    "- [x] Generate 6 dataset subsets, containing timesteps each (i.e., 30 time steps / 5 time steps per file)\n",
    "- [x] Generate one point spread function per dataset subset, for analysis of different\n",
    "- [x] Generate one point spread function for the whole dataset\n",
    "- [x] Determine if there is a significant difference between a subset PSF and the full dataset PSF - this will be the blur kernel\n",
    "- [ ] Determine if the PSF can be reduced (i.e., dont use the full PSF) - what effects might this have?\n",
    "- [x] Generate an IDFT of the full dataset, $L^2$ pixels\n",
    "- [x] Genetate an IDFT for each of the subsets, $M^2$ pixels\n",
    "- [x] Come up with a formula for populating the decimation matrix\n",
    "- [ ] Come up with a formula for populating the blur matrix \n",
    "- [ ] Determine if there is a formula for populating the product of the blur and decimation matrix, such that $H^TD^T = (HD)^T$\n",
    "- [ ] take the average of optimal betas across support 20 to 47, use that for all supports, plot the error between solved x and true x using the averaged beta, and compare that against another plot which is the \"optimal\" beta vs support\n",
    "- [ ] another thing to try, consider trimming the edge of solved x before taking norms and measuring RRMSE, as the convolution matrices are not padded and so convolution is essentially \"incomplete\" around the image edges. Probably only need to take say 10-15 pixels off all edges, since most of the PSF is noise aside from the center.\n",
    "- [ ] Another thing to try is to produce a set of Y images which are only representative of a single source, like a PSF. Then I could run the SOLE algorithm to determine what the effects of solving are against the curve of a point source, by taking a middle slice of the image (like I do when inspecting the actual PSF). Could examine what happens when using different supports and beta terms to see which lines up best with the true curve.\n",
    " - [ ] I need to modify the decimation matrix function to support more than just 4 neighbours (or 1/2 decimation).\n",
    " - [ ] Modify the project so it is more like a library, where I can call my own functions in my desired order. This would make repeating experiments easier than one flat notebook.\n",
    " - [ ] Implement the laplacian of gaussian function, so that I can have a dynamically sized laplacian instead of a fixed 3x3 (note: requires another config to specify laplacian support size, and a sigma value to determine curve spread, or are they roughly the same type of thing?)\n",
    " - [ ] Try running the algorithm with the averaged beta, using different sized LoGs and different PSF supports to see what happens re: rrmse\n",
    " - [ ] Figure out how I can get the sigma using a point of inflection of a large PSF (something like that?)\n",
    " - [ ] Implement assertion to confirm number of y samples and decimation factor follow critical formula from adaptive filtering approach paper\n",
    " - [ ] June 9 - Andrew suggests updating my decimation routine to be more dynamic, supporting powers of 2 or even arbitrary amounts, not sure how hard that is? He suggests experimenting with variable decimation and using a fixed optimal beta, fixed optimal sigma, and fixed PSF support (one of the higher end supports which seem stable, say 51? Could probably even just do the full PSF of 99x99 realistically\n",
    "\n",
    "---\n",
    "\n",
    "#### Notes:\n",
    "\n",
    "- If one takes the $\\beta S^T S$ portion of $A$ and multiplies it by the solved $X$;  assuming the result is not normalised, one can observe the contribution of beta-influenced laplacian sharpening towards $B$. It appears to float around 10% of the second portion of $A$ (the decimated PSF) multiplied by the solved $X$. Taking the sum of these two separate portions of $AX$ allow one to produce $B$. It appears that the laplacian sharpening portion of $A$ does meaningfully contribute to help find a reasonable solution to $X$, and doesnt appear to be dominating (i.e., drowning out) the decimated PSF portion of $A$.\n",
    "- Essentially, we dont want the beta-driven laplacian to dominate the sharpened PSF, otherwise you end up with effectively an overly smooth image $X$, and the technique becomes less useful / less impressive. We want to solve for $X$, such that beta helps us resolve a more accurate image $X$ which isnt just noise (a lack of structure, looks like static), and also doesnt look too smooth. If solving was less useful because we required large beta terms, then we might as well just average our set of $Y$ low-resolution images instead since its computationally cheaper?\n",
    " - Implement nifty gridder as a jupyter notebook - Andrew wants to see an image of true x, solved x, and nifty x\n",
    " - Andrew suggestions that in the future, a good method for analysing the effectiveness of this technique would be to compare the extractable sources and compare against a sky model (possibly Fred Dulwich has this?). It seems RRMSE is \"misleading\", in the sense that images produced by this technique arent so noisy, sources appear more gaussian-like and less like the dirty beam.\n",
    " - Furthermore, I should have a look at how this technique compares against the direct image model, versus an image produced by means of gridding (i.e., w-projection, nifty).\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58702d96-425b-4e12-8018-899e631892b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import cupy as cp\n",
    "from cupyx.scipy.sparse.linalg import cg as cg_gpu\n",
    "from cupyx.scipy.sparse.linalg import aslinearoperator as aslinearoperator_gpu\n",
    "from cupyx.scipy.sparse import csr_matrix as csr_matrix_gpu\n",
    "# from cupyx.scipy.sparse.linalg import lsqr as cuda_sparse_solve\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.signal import convolve2d\n",
    "from scipy.stats import norm\n",
    "from scipy.sparse import csr_matrix, bsr_matrix, dia_matrix, diags, csr_array\n",
    "from scipy.sparse.linalg import bicg, bicgstab, cg, cgs, gmres, lgmres, minres, qmr, gcrotmk, tfqmr, lsqr, lsmr, aslinearoperator\n",
    "from scipy.linalg import solve\n",
    "from skimage import data, io, color\n",
    "from skimage.transform import resize\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# from sparse_dot_mkl import dot_product_mkl as parallel_mult\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [15, 15]\n",
    "\n",
    "def matrix_memory(m):\n",
    "    return (m.data.nbytes + m.indptr.nbytes + m.indices.nbytes) / 10**6 # MB\n",
    "\n",
    "def matrix_density(m):\n",
    "    return m.getnnz() / np.prod(m.shape)\n",
    "\n",
    "def show_image(image, title, flip_x_axis=False):\n",
    "    if flip_x_axis:\n",
    "        image = np.fliplr(image)\n",
    "    plt.imshow(image, cmap=plt.get_cmap(\"gray\"))\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "    \n",
    "def show_iter_solution(x):\n",
    "    plt.imshow(np.fliplr(x.reshape(l, l)), cmap=plt.get_cmap(\"gray\"))\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "    \n",
    "def show_sparse_matrix(image, title):\n",
    "    plt.spy(image, markersize=5)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    \n",
    "def normalise(data):\n",
    "    return (data - np.min(data)) / (np.max(data) - np.min(data))\n",
    "\n",
    "def rrmse(observed, ideal, decimal=6):\n",
    "    return \"{:.{}f}\".format(np.sqrt((1 / observed.shape[0]**2) * np.sum((observed-ideal)**2) / np.sum(ideal**2)) * 100.0, decimal)\n",
    "\n",
    "def laplacian_of_gaussian(x, y, sigma):\n",
    "    p = (x**2.0 + y**2.0) / 2.0 * sigma**2.0\n",
    "    return -(1.0 / (np.pi * sigma**4.0)) * (1.0 - p) * np.exp(-p)\n",
    "\n",
    "# Generates a sparse decimation matrix using the Block Sparse Row matrix format (https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.bsr_matrix.html#scipy-sparse-bsr-matrix)\n",
    "def decimation_matrix(original_dim, downsample_factor):\n",
    "    \n",
    "    if original_dim % downsample_factor != 0:\n",
    "        raise ValueError(f\"Downsample factor {downsample_factor} is not a valid factor of your matrix dimension {original_dim}.\")\n",
    "    if downsample_factor == original_dim:\n",
    "        raise ValueError(f\"Downsample factor {downsample_factor} cannot be the same as your matrix dimension {original_dim}.\")\n",
    "    if downsample_factor == 1: # effectively, no downsampling\n",
    "        return np.identity(original_dim**2)\n",
    "    # Otherwise assumed you want to downsample by a valid factor of original_dim...\n",
    "    \n",
    "    sampling_regions_per_dim = original_dim // downsample_factor\n",
    "    samples_per_region_dim = downsample_factor\n",
    "    # print(f\"Sampling regions per dimension: {sampling_regions_per_dim}\")\n",
    "    # print(f\"Samples per dimension: {samples_per_region_dim}\")\n",
    "    non_zero_entries = sampling_regions_per_dim**2 * samples_per_region_dim**2\n",
    "    # print(f\"Non-zero entries: {non_zero_entries}\")\n",
    "    \n",
    "    rows = np.zeros(non_zero_entries, dtype=np.uintc)   # stores row indices for non-zero compressed sparse matrix entries\n",
    "    cols = np.zeros(non_zero_entries, dtype=np.uintc)   # stores col indices for non-zero compressed sparse matrix entries\n",
    "    vals = np.ones(non_zero_entries, dtype=np.float32)  # stores element value at [row, col] for non-zero entries\n",
    "    \n",
    "    # Generates linear x,y index strides for downsampling\n",
    "    sample_stride_1D = np.arange(0, original_dim, downsample_factor)\n",
    "    # print(sample_stride_1D)\n",
    "    mesh = np.array(np.meshgrid(sample_stride_1D, sample_stride_1D))\n",
    "    sample_strides_2D = mesh.T.reshape(-1, 2)\n",
    "  \n",
    "    neighbour_strides_1D = np.arange(samples_per_region_dim)\n",
    "    neighbour_mesh = np.array(np.meshgrid(neighbour_strides_1D, neighbour_strides_1D))\n",
    "\n",
    "    for index in np.arange(sample_strides_2D.shape[0]):\n",
    "        neighbour_coords = neighbour_mesh.T.reshape(-1, 2) + sample_strides_2D[index] # generates (row, col) index pair for the nxn neighbours of each sampling point in sample_strides_2D\n",
    "        neighbour_coords[:, 0] *= original_dim # scale y coord by high-resolution image dim to enable row striding (due to column-vector matrix flattening)\n",
    "        neighbour_coords = np.sum(neighbour_coords, axis=1) # combine x and y coord into single array index\n",
    "        rows[index * neighbour_coords.shape[0] : (index + 1) * neighbour_coords.shape[0]] = index\n",
    "        cols[index * neighbour_coords.shape[0] : (index + 1) * neighbour_coords.shape[0]] = neighbour_coords\n",
    "        \n",
    "    return csr_matrix((vals, (rows, cols)))\n",
    "\n",
    "def conv_matrix(l, kernel):\n",
    "    k_supp = kernel.shape[0]\n",
    "    k_half_supp = (k_supp-1)//2\n",
    "    k_samples = k_supp**2\n",
    "    col_offsets = np.repeat(np.arange(k_supp) - k_half_supp, k_supp) * (l - k_supp)\n",
    "    diagonal_offsets = (np.arange(k_samples) - (k_samples-1)//2) + col_offsets\n",
    "    m = diags(kernel.flatten(), diagonal_offsets, shape=(l**2, l**2), format=\"bsr\", dtype=np.float32)\n",
    "    \n",
    "    mask_vals = np.repeat(1.0, k_supp)\n",
    "    mask_offsets = np.linspace(-k_half_supp, k_half_supp, k_supp, dtype=np.intc)\n",
    "    mask = diags(mask_vals, mask_offsets, shape=(l, l), format=\"bsr\", dtype=np.float32)\n",
    "    \n",
    "    try:\n",
    "        Parallel(n_jobs=-1, prefer='threads')(delayed(apply_mask)(row, l, k_half_supp, m, mask) for row in range(l))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "    return m  \n",
    "\n",
    "def apply_mask(r, l, k_half_supp, m, mask):\n",
    "    col_chunk_min = max(0, r-k_half_supp-1)\n",
    "    col_chunk_max = min(l-1, r+k_half_supp+1)\n",
    "    for c in range(col_chunk_min, col_chunk_max+1): # need to process only partial column chunks, probably needs parallelisation...\n",
    "        m[r*l:r*l+l, c*l:c*l+l] = m[r*l:r*l+l, c*l:c*l+l].multiply(mask)\n",
    "    \n",
    "def generate_d_matrix(l, decimation_factor):\n",
    "    return decimation_matrix(l, decimation_factor)\n",
    "    \n",
    "def generate_h_matrix(l, kernel):\n",
    "    h = conv_matrix(l, kernel)\n",
    "    print(f\"H matrix memory usage: {matrix_memory(h)}MB\")\n",
    "    print(f\"H matrix density: {matrix_density(h)}%\")\n",
    "    return h \n",
    "    \n",
    "def generate_s_matrix(l, kernel=None, use_laplacian_of_gaussian=False):\n",
    "\n",
    "    # default to 3x3 laplacian\n",
    "    if kernel is None:\n",
    "        # kernel = np.array([[0, -1,  0], [-1,  4, -1], [0, -1,  0]], dtype=np.float32)\n",
    "        kernel = np.array([[-1, -1,  -1], [-1,  8, -1], [-1, -1,  -1]], dtype=np.float32)\n",
    "    \n",
    "    # Sharpening matrix (laplacian)\n",
    "    if use_laplacian_of_gaussian:\n",
    "        psf_central_support = 15 # num pixels around centre of PSF which represent approx 1/3 of the curve, maybe a bit bigger for padding\n",
    "        downsampling = np.linspace(-(psf_central_support-1)//2, (psf_central_support-1)//2, num=psf_central_support)\n",
    "        lap_of_gauss = np.zeros((downsampling.shape[0], downsampling.shape[0]), np.float32)\n",
    "        lap_gauss_σ = 2.205128205 # need to play around with this to get similar structure to true laplacian 3x3\n",
    "        for i in np.arange(downsampling.shape[0]):\n",
    "            for j in np.arange(downsampling.shape[0]):\n",
    "                lap_of_gauss[i][j] = -laplacian_of_gaussian(downsampling[i], downsampling[j], lap_gauss_σ)\n",
    "\n",
    "        kernel = lap_of_gauss / np.max(lap_of_gauss)\n",
    "\n",
    "        plt.plot(lap_of_gauss[lap_of_gauss.shape[0]//2])\n",
    "        plt.show()\n",
    "\n",
    "        # β *= 4.0\n",
    "\n",
    "    s = conv_matrix(l, kernel)\n",
    "    # print(type(s))\n",
    "    # print(f\"S matrix memory usage: {matrix_memory(s)}MB\")\n",
    "    # print(f\"S matrix density: {matrix_density(s)}%\")\n",
    "    # show_sparse_matrix(s, \"S\")\n",
    "    \n",
    "    return s\n",
    "    \n",
    "def generate_b_image(n, w, dh, y):\n",
    "    b = np.zeros(l**2, dtype=np.float32)\n",
    "\n",
    "    for i in np.arange(n):\n",
    "        b += w[i] * csr_matrix.transpose(dh) @ y[i].flatten()\n",
    "        \n",
    "    # print((b.size * b.itemsize) / 10**6)\n",
    "    \n",
    "    return b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b98346-6318-477d-8555-44909edbde9c",
   "metadata": {},
   "source": [
    "#### Configuration and data set up..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a26282b-1625-4678-bfde-510ae1a36b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PSF min/max/support: 175, 226, 51\n",
      "l^2:                      160000\n",
      "(psf_support - 2) * m^2:  490000\n",
      "n * m^2:                  300000\n",
      "Super-resolution imaging technically possible, continuing...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "timesteps = 30 # total timesteps\n",
    "timesteps_per_y = 1\n",
    "l = 400\n",
    "m = 100\n",
    "n = timesteps // timesteps_per_y\n",
    "w = np.ones(n)\n",
    "β = 10.0 #0.550868006556238\n",
    "blur_kernel_support = 51\n",
    "\n",
    "# Note: flowchart for selecting solver routine might be useful..\n",
    "# https://www.mathworks.com/help/matlab/math/iterative-methods-for-linear-systems.html\n",
    "solver = gcrotmk\n",
    "\n",
    "# False = use static 3x3 laplacian kernel\n",
    "# True = use customisable laplacian of gaussian kernel\n",
    "use_laplacian_of_gaussian = False\n",
    "\n",
    "# False = solve using CPU based scipy library\n",
    "# True = solve using GPU based cupy library\n",
    "use_accelerated_solving = False\n",
    "\n",
    "# all time steps direct image\n",
    "filename = f\"../datasets/gleam_small/images/single_source/direct_image_ts_0_29_{l}x{l}.bin\"\n",
    "x_true = normalise(np.fromfile(filename, dtype=np.float32).reshape(l, l))\n",
    "# show_image(x_true, \"True X\")\n",
    "\n",
    "filename = f\"../datasets/gleam_small/images/single_source/direct_psf_ts_0_29_{l}x{l}.bin\"\n",
    "x_psf = np.fromfile(filename, dtype=np.float32).reshape(l, l)\n",
    "# show_image(x_psf, \"Untrimmed PSF\")\n",
    "# plt.plot(x_psf[x_psf.shape[0]//2])\n",
    "# plt.show()\n",
    "\n",
    "blur_kernel_support_half = (blur_kernel_support - 1) // 2\n",
    "trim_min = l//2 - (blur_kernel_support_half)\n",
    "trim_max = l//2 + blur_kernel_support_half + 1\n",
    "print(f\"PSF min/max/support: {trim_min}, {trim_max}, {blur_kernel_support}\")\n",
    "# x_psf = x_psf.reshape(l, l)[psf_min:psf_max, psf_min:psf_max]\n",
    "x_psf_trimmed = x_psf.copy()[trim_min : trim_max, trim_min : trim_max]\n",
    "x_psf_trimmed /= np.sum(x_psf_trimmed)\n",
    "# print(np.sum(x_psf_trimmed))\n",
    "# plt.plot(x_psf_trimmed[x_psf_trimmed.shape[0]//2])\n",
    "# plt.show()\n",
    "\n",
    "# print(f\"PSF Shape: {x_psf_trimmed.shape[0]}\")\n",
    "# show_image(x_psf_trimmed, \"PSF\")\n",
    "\n",
    "print(f\"l^2:                      {l**2}\")\n",
    "print(f\"(psf_support - 2) * m^2:  {(blur_kernel_support - 2) * m**2}\")\n",
    "print(f\"n * m^2:                  {n * m**2}\")\n",
    "\n",
    "# Determine whether we have the right criteria to perform super-resolution imaging to obtain X\n",
    "if l**2 <= np.minimum((blur_kernel_support - 2) * m**2, n * m**2):\n",
    "    print(\"Super-resolution imaging technically possible, continuing...\\n\")\n",
    "else:\n",
    "    raise ValueError(\"High-resolution image not possible to reproduce from your set up, review your configuration against this if statement conditional.\\n\")\n",
    "\n",
    "# Storing all low-res images as layered stack\n",
    "y = np.zeros((n, m, m))\n",
    "\n",
    "# batched time steps direct images\n",
    "for i in np.arange(n):\n",
    "    timestep_range_start = i * timesteps_per_y\n",
    "    timestep_range_end = timestep_range_start + timesteps_per_y\n",
    "    filename = f\"../datasets/gleam_small/images/single_source/direct_image_ts_{timestep_range_start}_{timestep_range_end - 1}_{m}x{m}.bin\"\n",
    "    y[i] = np.fromfile(filename, dtype=np.float32).reshape(m, m)\n",
    "    y[i] = normalise(y[i])\n",
    "\n",
    "s = generate_s_matrix(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fd6a15f-0815-4697-a031-391d2eba1650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "H matrix memory usage: 2014.656196MB\n",
      "H matrix density: 0.01851559%\n",
      "CPU times: user 23.6 s, sys: 3.58 s, total: 27.2 s\n",
      "Wall time: 27.2 s\n",
      "CPU times: user 3.03 s, sys: 423 ms, total: 3.46 s\n",
      "Wall time: 3.46 s\n",
      "DH matrix memory usage: 225.457604MB\n",
      "DH matrix density: 0.01761075%\n"
     ]
    }
   ],
   "source": [
    "%time h = generate_h_matrix(l, x_psf_trimmed)\n",
    "\n",
    "# Decimation matrix\n",
    "d = generate_d_matrix(l, 4)\n",
    "\n",
    "# show_sparse_matrix(d, \"D\")\n",
    "# d_numpy_size_bytes = d.size * d.itemsize\n",
    "# print(f\"D matrix uncompressed sparse: {d_numpy_size_bytes / 10**6} MB\")\n",
    "# d = bsr_matrix(d)\n",
    "# print(f\"D matrix memory usage: {matrix_memory(d)}MB\")\n",
    "# print(f\"D matrix density: {matrix_density(d)}%\")\n",
    "# print(f\"D matrix reduced by {100 - (d_csr_matrix_bytes / d_numpy_size_bytes * 100)}%\\n\")\n",
    "\n",
    "# dh = parallel_mult(d, h)\n",
    "%time dh = d @ h\n",
    "# show_sparse_matrix(dh, \"DH\")\n",
    "\n",
    "print(f\"DH matrix memory usage: {matrix_memory(dh)}MB\")\n",
    "print(f\"DH matrix density: {matrix_density(dh)}%\")\n",
    "    \n",
    "# Notes to self: seems like lap_gauss_σ sits around 1.41 for this example, any higher or lower results in worse RRMSE. Changing the psf_central_support makes no difference, and taking more samples (i.e. num)\n",
    "# just means that lap_gauss_σ is multiplied by the factor of difference (i.e., double the num samples needs double the lap_gauss_σ). Also found that beta is best multiplied by 4.0, not 16 (the square).\n",
    "# Not sure what to do from here... seems like the laplacian of gaussian doesnt really serve any beneficial purpose..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f5877a-f42d-4bbe-9815-09fba1baae3f",
   "metadata": {},
   "source": [
    "#### Setting up the right hand side of the equation $AX = B$, where $B = \\sum\\limits_{i=1}^N (w_iH^TD^TY_i)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e70b5b39-97ff-454d-a18c-be18ca528ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = generate_b_image(n, w, dh, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42023895-557c-4c97-a10e-bf1a2b43d09a",
   "metadata": {},
   "source": [
    "#### Setting up the right hand side of the equation $AX=B$, where $A = \\begin{bmatrix}\n",
    "  \\beta S^T S + (\\sum\\limits_{i=1}^N w_i) H^TD^TDH\n",
    "\\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8cad2eee-2322-4931-825f-0efa8ca8c419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %time lhs = (β * (s @ csr_matrix.transpose(s)))\n",
    "# print(f\"A_LHS l2 norm: {np.sum(lhs**2)}\")\n",
    "# print(f\"A_LHS matrix memory usage: {matrix_memory(lhs)}MB\")\n",
    "# print(f\"A_LHS matrix density: {matrix_density(lhs)}%\")\n",
    "\n",
    "# # This section is painfully slow... like 3-4 mins\n",
    "# %time rhs = ((csr_matrix.transpose(dh) @ dh) * np.sum(w))\n",
    "# print(f\"A_RHS matrix memory usage: {matrix_memory(rhs)}MB\")\n",
    "# print(f\"A_RHS matrix density: {matrix_density(rhs)}%\")\n",
    "\n",
    "# %time a = lhs + rhs\n",
    "# print(f\"A matrix memory usage: {matrix_memory(a)}MB\")\n",
    "# print(f\"A matrix density: {matrix_density(a)}%\")\n",
    "\n",
    "# runtime for scipy mat mul = 4 min 26 sec\n",
    "# runtime for parallel mult = \n",
    "\n",
    "# a = bsr_matrix.toarray(a)\n",
    "# show_image(a.reshape(l**2, l**2), \"A\")\n",
    "# print(f\"A l2 norm: {np.linalg.norm(a)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f646df-d788-465a-b6b6-f0a308d22e9a",
   "metadata": {},
   "source": [
    "#### Now solve for $X$..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c87506-72dd-42e6-8e3e-c83c821d9a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "use_accelerated_solving = False\n",
    "\n",
    "if use_accelerated_solving:\n",
    "    # CUPY WORK START\n",
    "    s_gpu = csr_matrix_gpu(s)\n",
    "    dh_gpu = csr_matrix_gpu(dh)\n",
    "    slo_gpu = aslinearoperator_gpu(s_gpu) # does s need to be put onto gpu?\n",
    "    dhlo_gpu = aslinearoperator_gpu(dh_gpu) # does dh need to be put onto gpu?\n",
    "    b_gpu = cp.array(b)\n",
    "    w_sum = np.sum(w)\n",
    "    alo_gpu = (β * (slo_gpu.T @ slo_gpu)) + ((dhlo_gpu.T @ dhlo_gpu) * w_sum)\n",
    "    xlo_gpu, info = cg_gpu(alo_gpu, b_gpu, maxiter=100)\n",
    "    # print(f\"GPU INFO: {info}\")\n",
    "    xlo = cp.asnumpy(xlo_gpu).reshape(l, l)\n",
    "    # CUPY WORK END\n",
    "else:\n",
    "    slo = aslinearoperator(s)\n",
    "    dhlo = aslinearoperator(dh)\n",
    "    alo = (β * (slo.T @ slo)) + ((dhlo.T @ dhlo) * np.sum(w))\n",
    "    xlo, info = solver(alo, b, maxiter=15, callback=show_iter_solution) #, callback_type='x')\n",
    "    xlo = xlo.reshape(l, l)\n",
    "    # print(f\"INFO: {info}\")\n",
    "\n",
    "# x, _ = solver(a, b, maxiter = 1)\n",
    "# x = x.reshape(l, l)\n",
    "# show_image(normalise(x), \"Solved X\", flip_x_axis=True)\n",
    "# print(f\"Beta: {β}\")\n",
    "# print(f\"RRMSE: Solved X and True X -> {rrmse(normalise(x), normalise(x_true))}\")\n",
    "# print(f\"RRMSE: Solved X and B -> {rrmse(normalise(x), normalise(b.reshape(l, l)))}\")\n",
    "# print(f\"True X peak location: {np.unravel_index(np.argmax(x_true, axis=None), x_true.shape)}\")\n",
    "# print(f\"Solved X peak location: {np.unravel_index(np.argmax(x, axis=None), x.shape)}\")\n",
    "\n",
    "# show_image(normalise(xlo), \"Solved X (Linear Operator)\", flip_x_axis=True)\n",
    "# show_image(normalise(x_true), \"True X\", flip_x_axis=True)\n",
    "\n",
    "print(f\"Beta: {β}\")\n",
    "print(f\"RRMSE: Solved X and True X -> {rrmse(normalise(xlo), normalise(x_true))}\")\n",
    "print(f\"RRMSE: Solved X and B -> {rrmse(normalise(xlo), normalise(b.reshape(l, l)))}\")\n",
    "print(f\"RRMSE: B and True X -> {rrmse(normalise(b.reshape(l, l)), normalise(x_true))}\")\n",
    "print(f\"True X peak location: {np.unravel_index(np.argmax(x_true, axis=None), x_true.shape)}\")\n",
    "print(f\"Solved X peak location: {np.unravel_index(np.argmax(xlo, axis=None), xlo.shape)}\")\n",
    "\n",
    "plt.plot(normalise(xlo[202, :]))\n",
    "plt.plot(normalise(x_true[200, :]))\n",
    "plt.show()\n",
    "\n",
    "# print(rrmse(normalise(x), normalise(xlo)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927dde27-f9e5-4adf-bfaa-886e260db6cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

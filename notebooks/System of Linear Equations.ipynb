{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ef38024-d513-436b-9b95-7996a4ec69c1",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "### Simplified System of linear equations solving for image $X$\n",
    "\n",
    "Key terms:\n",
    "\n",
    "$L$ = one-dimensional size for high-resolution image\n",
    "\n",
    "$M$ = one-dimensional size for low-resolution image\n",
    "\n",
    "$X$ = $L^2 \\times 1$ column vector, represents the ideal image we are trying to recover via solving\n",
    "\n",
    "$Y_i$ = $M^2 \\times 1$ column vector, represents a decimated, downsampled, blurred, noisy image based on $X$\n",
    "\n",
    "$N$ = the number of low-resolution images\n",
    "\n",
    "$D$ = the decimation matrix operator of size $M^2 \\times L^2$\n",
    "\n",
    "$H$ = the blur matrix operator of size $L^2 \\times L^2$\n",
    "\n",
    "$S$ = the sharpening operator of size $L^2 \\times L^2$, i.e., the laplacian operator\n",
    "\n",
    "$w_i$ = a weighting scalar, a \"confidence factor\" for each $Y_i$ (eventually a diagonal matrix?)\n",
    "\n",
    "$\\beta$ = smoothing factor for controlling image sharpening\n",
    "\n",
    "---\n",
    "\n",
    "Objective: to solve for $X$ (i.e., $AX = B$) based on the following equation:\n",
    "\n",
    "$\\begin{bmatrix}\n",
    "  \\beta S^T S + (\\sum\\limits_{i=1}^N w_i) H^TD^TDH\n",
    "\\end{bmatrix}X = \\sum\\limits_{i=1}^N (w_iH^TD^TY_i)$\n",
    "\n",
    "---\n",
    "\n",
    "General information:\n",
    "\n",
    "- Dataset: GLEAM Small\n",
    "- Timesteps: 30\n",
    "- Receivers: 512\n",
    "- Channels: 1\n",
    "- $L$ = 100 pixels\n",
    "- $M$ = 50 pixels\n",
    "- $N$ = 6 images (30 timesteps / 5 timesteps per image)\n",
    "- $w_i$ = 1 for all images\n",
    "- $\\beta$ = 1 (I guess?)\n",
    "\n",
    "Asumptions:\n",
    "1. The blur kernel $H$ is uniform across all instances of $Y$\n",
    "2. The decimatimation kernel $D$ is uniform across all instances of $Y$\n",
    "3. The $w_i$ is uniform across all instances of $Y$, just a confidence of 1.0 (\"full confidence\")\n",
    "\n",
    "---\n",
    "\n",
    "#### To do:\n",
    "\n",
    "- [x] Generate 6 dataset subsets, containing timesteps each (i.e., 30 time steps / 5 time steps per file)\n",
    "- [x] Generate one point spread function per dataset subset, for analysis of different\n",
    "- [x] Generate one point spread function for the whole dataset\n",
    "- [x] Determine if there is a significant difference between a subset PSF and the full dataset PSF - this will be the blur kernel\n",
    "- [ ] Determine if the PSF can be reduced (i.e., dont use the full PSF) - what effects might this have?\n",
    "- [x] Generate an IDFT of the full dataset, $L^2$ pixels\n",
    "- [x] Genetate an IDFT for each of the subsets, $M^2$ pixels\n",
    "- [x] Come up with a formula for populating the decimation matrix\n",
    "- [ ] Come up with a formula for populating the blur matrix \n",
    "- [ ] Determine if there is a formula for populating the product of the blur and decimation matrix, such that $H^TD^T = (HD)^T$\n",
    "- [ ] take the average of optimal betas across support 20 to 47, use that for all supports, plot the error between solved x and true x using the averaged beta, and compare that against another plot which is the \"optimal\" beta vs support\n",
    "- [ ] another thing to try, consider trimming the edge of solved x before taking norms and measuring RRMSE, as the convolution matrices are not padded and so convolution is essentially \"incomplete\" around the image edges. Probably only need to take say 10-15 pixels off all edges, since most of the PSF is noise aside from the center.\n",
    "- [ ] Another thing to try is to produce a set of Y images which are only representative of a single source, like a PSF. Then I could run the SOLE algorithm to determine what the effects of solving are against the curve of a point source, by taking a middle slice of the image (like I do when inspecting the actual PSF). Could examine what happens when using different supports and beta terms to see which lines up best with the true curve.\n",
    " - [ ] I need to modify the decimation matrix function to support more than just 4 neighbours (or 1/2 decimation).\n",
    " - [ ] Modify the project so it is more like a library, where I can call my own functions in my desired order. This would make repeating experiments easier than one flat notebook.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58702d96-425b-4e12-8018-899e631892b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "# import cupy as cp\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import convolve2d\n",
    "from skimage import data, io, color\n",
    "from skimage.transform import resize\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [10, 10]\n",
    "\n",
    "def show_image(image, title, flip_x_axis=False):\n",
    "    if flip_x_axis:\n",
    "        image = np.fliplr(image)\n",
    "    plt.imshow(image, cmap=plt.get_cmap(\"gray\"))\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "    \n",
    "def normalise(data):\n",
    "    return (data - np.min(data)) / (np.max(data) - np.min(data))\n",
    "\n",
    "def rrmse(observed, ideal, decimal=6):\n",
    "    return \"{:.{}f}\".format(np.sqrt((1 / observed.shape[0]**2) * np.sum((observed-ideal)**2) / np.sum(ideal**2)) * 100.0, decimal)\n",
    "\n",
    "def decimation_matrix(l, m):\n",
    "    d_matrix = np.zeros((m**2, l**2), dtype=np.float32)\n",
    "\n",
    "    tile = np.repeat((1, 0, 1), (2, l - 2, 2)) # assuming taking 2 neighbours per dimension\n",
    "    t_len = tile.shape[0]\n",
    "    d = l // m\n",
    "    r_offset = m**2 // 2\n",
    "    c_offset = l**2 // 2\n",
    "\n",
    "    for p in np.arange(l//4): # divide by 4 as 4 neighbours total\n",
    "        p_offset = p * l\n",
    "        for q in np.arange(m):\n",
    "            d_matrix[q+ p_offset//2, q*d + p_offset*2 : q*d+t_len + p_offset*2] = tile # top-left quadrant\n",
    "            d_matrix[q+r_offset + p_offset//2, q*d+c_offset + p_offset*2: q*d+t_len+c_offset + p_offset*2] = tile # bottom-right quadrant\n",
    "    return d_matrix\n",
    "\n",
    "# produces convolution matrix of size l**2 by l**2, where each row is populated by the convolution kernel values at the appropriate neighbours\n",
    "# note: assumes kernel is a two-dimensonal numpy array of some size n by n\n",
    "def convolution_matrix(l, kernel):\n",
    "    \n",
    "    conv = np.zeros((l**2, l**2), dtype=np.float32)\n",
    "    full_supp = kernel.shape[0] # assumed square\n",
    "    half_supp = (full_supp - 1) // 2\n",
    "\n",
    "    for conv_row in np.arange(l**2):\n",
    "\n",
    "        row, col = (conv_row // l, conv_row % l)\n",
    "\n",
    "        for k_row in np.arange(-(half_supp), half_supp + 1):\n",
    "            # map \"kernel row\" to rows in conv\n",
    "            mapped_row = row + k_row\n",
    "            # ignore any out of bounds rows\n",
    "            if mapped_row >= 0 and mapped_row < l:\n",
    "                linear_col = col - half_supp\n",
    "                # truncate negative columns\n",
    "                mapped_col_start = max(linear_col, 0)\n",
    "                # truncate columns which exceed the l dimension\n",
    "                mapped_col_end = min(linear_col + full_supp, l)\n",
    "                # left trimming for kernels when overlapping out of bounds region in conv (col < 0)\n",
    "                left = np.absolute(col - half_supp) if linear_col < 0 else 0\n",
    "                # right trimming for kernels when overlapping out of bounds region in conv (col >= l)\n",
    "                right = linear_col + full_supp - l if linear_col + full_supp >= l else 0 \n",
    "                # copy over kernel row for current k_row, possibly including trimming for out of bounds coordinates\n",
    "                conv[conv_row][mapped_row * l + mapped_col_start : mapped_row * l + mapped_col_end] = kernel[k_row + half_supp][left: left + full_supp - right]\n",
    "    return conv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b98346-6318-477d-8555-44909edbde9c",
   "metadata": {},
   "source": [
    "#### Configuration and data set up..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a616972-ec82-4c15-b1b1-898565a8fc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps = 30 # total timesteps\n",
    "timesteps_per_y = 5\n",
    "l = 100\n",
    "m = 50\n",
    "n = timesteps // timesteps_per_y\n",
    "w = np.ones(n)\n",
    "Î² = 0.203091762\n",
    "\n",
    "# all time steps direct image\n",
    "# filename = \"../data/direct_image_ts_0_29.bin\"\n",
    "filename = \"../data/direct_image_ts_0_29_800x800.bin\"\n",
    "x_true = np.fromfile(filename, dtype=np.float32)\n",
    "# x_true = np.reshape(x_true, (l, l))\n",
    "x_true = resize(x_true.reshape(800, 800), (l, l), anti_aliasing=False, order=1)\n",
    "x_true = normalise(x_true)\n",
    "# show_image(x_true, \"True X (800^2 averaged to 100^2)\")\n",
    "\n",
    "# x_orig = np.fromfile(\"../data/direct_image_ts_0_29.bin\", dtype=np.float32)\n",
    "# show_image(normalise(x_orig.reshape(l, l)), \"Original True X (100^2)\")\n",
    "\n",
    "# all time steps direct psf\n",
    "# filename = \"../data/direct_psf_ts_0_29.bin\"\n",
    "# filename = \"../data/direct_psf_ts_0_29_800x800.bin\"\n",
    "# x_psf = np.fromfile(filename, dtype=np.float32)\n",
    "# x_psf = resize(x_psf.reshape(800, 800), (l, l), anti_aliasing=False, order=1)\n",
    "# x_psf = x_psf.reshape(l, l)[1:, 1:] # trim row 0 and all column 0 (to ensure odd dimensions with the peak in the center)\n",
    "# x_psf /= np.max(x_psf)\n",
    "# show_image(x_psf, \"PSF (800^2 averaged to 100^2)\")\n",
    "\n",
    "filename = \"../data/direct_psf_ts_0_29_800x800.bin\"\n",
    "x_psf = np.fromfile(filename, dtype=np.float32).reshape(800, 800)[1:, 1:]\n",
    "plt.plot(x_psf[x_psf.shape[0]//2])\n",
    "plt.show()\n",
    "# show_image(x_psf, \"Untrimmed PSF\")\n",
    "x_psf = resize(x_psf, (l-1, l-1), anti_aliasing=False, order=1)\n",
    "x_psf = np.pad(x_psf, ((1, 0), (1, 0))) # pad with new 0th row/col to ensure trimming from centre\n",
    "# show_image(x_psf, \"Untrimmed PSF - RESIZED\")\n",
    "# plt.plot(x_psf[x_psf.shape[0]//2])\n",
    "# plt.show()\n",
    "\n",
    "trim_half_len = 26\n",
    "psf_min = l//2 - (trim_half_len - 1)\n",
    "psf_max = l//2 + trim_half_len\n",
    "# x_psf = x_psf.reshape(l, l)[psf_min:psf_max, psf_min:psf_max]\n",
    "x_psf = x_psf[psf_min:psf_max, psf_min:psf_max]\n",
    "x_psf /= np.sum(x_psf)\n",
    "print(np.sum(x_psf))\n",
    "# plt.plot(x_psf[x_psf.shape[0]//2])\n",
    "# plt.show()\n",
    "\n",
    "print(f\"PSF Shape: {x_psf.shape[0]}\")\n",
    "# show_image(np.log(np.absolute(x_psf)), \"PSF\")\n",
    "\n",
    "# Storing all low-res images as layered stack\n",
    "y = np.zeros((n, m, m))\n",
    "\n",
    "# batched time steps direct images\n",
    "for i in np.arange(n):\n",
    "    filename = f\"../data/direct_image_ts_{i * timesteps_per_y}_{i * timesteps_per_y + timesteps_per_y - 1}.bin\"\n",
    "    y[i] = np.fromfile(filename, dtype=np.float32).reshape(m, m)\n",
    "    y[i] = normalise(y[i])\n",
    "    # show_image(y[i], f\"$Y_{i}$\", flip_x_axis=True)\n",
    "    \n",
    "# batched time steps point spread functions\n",
    "# for i in np.arange(N):\n",
    "#     start = i * timesteps_per_y\n",
    "#     end = start + timesteps_per_y - 1\n",
    "#     filename = \"../data/direct_psf_ts_%d_%d.bin\" % (start, end)\n",
    "#     Y_i_psf = np.fromfile(filename, dtype=np.float32)\n",
    "#     Y_i_psf = Y_i_psf.reshape(L, L)\n",
    "#     # show_image(Y_i_psf, \"$Y_{%d}$ PSF\" % i)\n",
    "\n",
    "# Decimation matrix\n",
    "d = decimation_matrix(l, m) # takes the sum of 4 l neighbours to form 1 m pixel\n",
    "\n",
    "# Blur matrix (psf)\n",
    "h = convolution_matrix(l, x_psf)\n",
    "\n",
    "# Sharpening matrix (laplacian)\n",
    "laplacian = np.array([[0, -1,  0], [-1,  4, -1], [0, -1,  0]], dtype=np.float32)\n",
    "s = convolution_matrix(l, laplacian)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f5877a-f42d-4bbe-9815-09fba1baae3f",
   "metadata": {},
   "source": [
    "#### Setting up the right hand side of the equation $AX = B$, where $B = \\sum\\limits_{i=1}^N (w_iH^TD^TY_i)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3eb9802-b912-440c-a441-2c212ebd9b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.zeros(l**2, dtype=np.float32)\n",
    "\n",
    "for i in np.arange(n):\n",
    "    b += np.matmul(w[i] * h.T, np.matmul(d.T, y[i].flatten()))\n",
    "                     \n",
    "show_image(normalise(b.reshape(l, l)), \"B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42023895-557c-4c97-a10e-bf1a2b43d09a",
   "metadata": {},
   "source": [
    "#### Setting up the right hand side of the equation $AX=B$, where $A = \\begin{bmatrix}\n",
    "  \\beta S^T S + (\\sum\\limits_{i=1}^N w_i) H^TD^TDH\n",
    "\\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35827ee1-0746-4887-8e72-441e3db0723a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lhs = Î² * np.matmul(s.T, s)\n",
    "rhs = (h.T @ d.T @ d @ h) * np.sum(w)\n",
    "a = lhs + rhs\n",
    "# ow_image(a.reshape(l**2, l**2), \"A\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f646df-d788-465a-b6b6-f0a308d22e9a",
   "metadata": {},
   "source": [
    "#### Now solve for $X$..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c87506-72dd-42e6-8e3e-c83c821d9a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solving via CPU and numpy...\n",
    "x = np.linalg.solve(a, b)\n",
    "x = x.reshape(100, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1855c1fa-46d8-467a-9405-a59a7fb648b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Solving via GPU and cupy\n",
    "# a_gpu = cp.asarray(a)\n",
    "# b_gpu = cp.asarray(b)\n",
    "# solution = cp.linalg.solve(a_gpu, b_gpu)\n",
    "# x = cp.asnumpy(solution)\n",
    "# x = x.reshape(100, 100)\n",
    "\n",
    "# # Dealloc cuda mem\n",
    "# mempool = cp.get_default_memory_pool()\n",
    "# mempool.free_all_blocks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6764572-2498-4772-8051-feb390d94eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# b_true = np.matmul(normalise(a), x_true.flatten())\n",
    "# x_accurate = np.linalg.solve(a, b_true)\n",
    "# x_accurate = x_accurate.reshape(100, 100)\n",
    "\n",
    "# show_image(normalise(x), \"Solved X\")\n",
    "# print(f\"RRMSE: B and True B -> {rrmse(normalise(b.reshape(l, l)), normalise(b_true.reshape(l, l)))}\")\n",
    "# print(f\"L2 between B and True X: {np.sqrt(np.sum((normalise(b.reshape(l, l))-normalise(x))**2))}\")\n",
    "# show_image(normalise(b.reshape(l, l)), \"B\")\n",
    "# show_image(normalise(b_true.reshape(l, l)), \"True B\")\n",
    "\n",
    "# print(f\"RRMSE: X and Accurate X -> {rrmse(normalise(x), normalise(x_accurate))}\")\n",
    "# print(f\"RRMSE: Accurate X and True X -> {rrmse(normalise(x_accurate), normalise(x_true))}\")\n",
    "print(f\"RRMSE: Solved X and True X -> {rrmse(normalise(x), normalise(x_true))}\")\n",
    "print(f\"Beta: {Î²}\")\n",
    "\n",
    "# Multiply A left hand side (beta and sharpening matrices) by X\n",
    "a_laplacian = lhs # np.matmul(lhs, x.flatten()).reshape(l, l)\n",
    "# show_image((a_laplacian), \"A-laplacian\", flip_x_axis=True)\n",
    "a_lap_l1 = np.max(np.sum(np.absolute(a_laplacian), axis=0))\n",
    "# a_lap_linf = np.max(np.sum(np.absolute(a_laplacian), axis=1))\n",
    "print(f\"A_laplacian l1 norm: {a_lap_l1}\")\n",
    "# print(f\"A_laplacian linf norm: {a_lap_linf}\")\n",
    "\n",
    "a_deci_blur = rhs # np.matmul(rhs, x.flatten()).reshape(l, l)\n",
    "# show_image((a_deci_blur), \"A-deci-blur\", flip_x_axis=True)\n",
    "a_dec_l1 = np.max(np.sum(np.absolute(a_deci_blur), axis=0))\n",
    "# a_dec_linf = np.max(np.sum(np.absolute(a_deci_blur), axis=1))\n",
    "print(f\"A_deci_blur l1 norm: {a_dec_l1}\")\n",
    "# print(f\"A_deci_blur linf norm: {a_dec_linf}\")\n",
    "\n",
    "print(f\"A_deci_blur / A_laplacian l1 ratio: {a_dec_l1 / a_lap_l1}\")\n",
    "# print(f\"A_deci_blur / A_laplacian linf ratio: {a_dec_linf / a_lap_linf}\")\n",
    "\n",
    "show_image(normalise(x), \"Solved X\", flip_x_axis=True)\n",
    "# show_image(normalise(x_accurate[10:l-11, 10:l-11]), \"Solved X using True X (IDFT)\")\n",
    "# show_image(normalise(x_accurate), \"Solved X using True X (IDFT)\")\n",
    "show_image(normalise(x_true), \"True X\", flip_x_axis=True)\n",
    "# show_image(normalise(a_laplacian + a_deci_blur), \"A parts summed to B\", flip_x_axis=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9346c3-3c37-43b5-b393-9e91a8c45756",
   "metadata": {},
   "source": [
    "#### Notes to self:\n",
    "\n",
    "- If one takes the $\\beta S^T S$ portion of $A$ and multiplies it by the solved $X$;  assuming the result is not normalised, one can observe the contribution of beta-influenced laplacian sharpening towards $B$. It appears to float around 10% of the second portion of $A$ (the decimated PSF) multiplied by the solved $X$. Taking the sum of these two separate portions of $AX$ allow one to produce $B$. It appears that the laplacian sharpening portion of $A$ does meaningfully contribute to help find a reasonable solution to $X$, and doesnt appear to be dominating (i.e., drowning out) the decimated PSF portion of $A$.\n",
    "- Essentially, we dont want the beta-driven laplacian to dominate the sharpened PSF, otherwise you end up with effectively an overly smooth image $X$, and the technique becomes less useful / less impressive. We want to solve for $X$, such that beta helps us resolve a more accurate image $X$ which isnt just noise (a lack of structure, looks like static), and also doesnt look too smooth. If solving was less useful because we required large beta terms, then we might as well just average our set of $Y$ low-resolution images instead since its computationally cheaper?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d332acb-b8dd-46c2-9f40-08df8793a94e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

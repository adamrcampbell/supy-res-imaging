{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ef38024-d513-436b-9b95-7996a4ec69c1",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "### Simplified System of linear equations solving for image $X$\n",
    "\n",
    "Key terms:\n",
    "\n",
    "$L$ = one-dimensional size for high-resolution image\n",
    "\n",
    "$M$ = one-dimensional size for low-resolution image\n",
    "\n",
    "$X$ = $L^2 \\times 1$ column vector, represents the ideal image we are trying to recover via solving\n",
    "\n",
    "$Y_i$ = $M^2 \\times 1$ column vector, represents a decimated, downsampled, blurred, noisy image based on $X$\n",
    "\n",
    "$N$ = the number of low-resolution images\n",
    "\n",
    "$D$ = the decimation matrix operator of size $M^2 \\times L^2$\n",
    "\n",
    "$H$ = the blur matrix operator of size $L^2 \\times L^2$\n",
    "\n",
    "$S$ = the sharpening operator of size $L^2 \\times L^2$, i.e., the laplacian operator\n",
    "\n",
    "$w_i$ = a weighting scalar, a \"confidence factor\" for each $Y_i$ (eventually a diagonal matrix?)\n",
    "\n",
    "$\\beta$ = smoothing factor for controlling image sharpening\n",
    "\n",
    "---\n",
    "\n",
    "Objective: to solve for $X$ (i.e., $AX = B$) based on the following equation:\n",
    "\n",
    "$\\begin{bmatrix}\n",
    "  \\beta S^T S + (\\sum\\limits_{i=1}^N w_i) H^TD^TDH\n",
    "\\end{bmatrix}X = \\sum\\limits_{i=1}^N (w_iH^TD^TY_i)$\n",
    "\n",
    "Note: In order to reconstruct a high-resolution image from $N$ low-resolution images, the following criteria must be satisfied: $L^2 \\leq \\min \\left\\{ \\left(s^2 - 2 \\right)M^2, NM^2\\right\\}$, where $s$ is the full support of the blurring kernel (i.e., point-spread function) (as per section III.B of [1]).\n",
    "\n",
    "[1] M. Elad and A. Feuer, \"Restoration of a single superresolution image from several blurred, noisy, and undersampled measured images,\" in IEEE Transactions on Image Processing, vol. 6, no. 12, pp. 1646-1658, Dec. 1997, doi: 10.1109/83.650118.\n",
    "\n",
    "---\n",
    "\n",
    "General information:\n",
    "\n",
    "- Dataset: GLEAM Small\n",
    "- Timesteps: 30\n",
    "- Receivers: 512\n",
    "- Channels: 1\n",
    "- $L$ = 100 pixels\n",
    "- $M$ = 50 pixels\n",
    "- $N$ = 6 images (30 timesteps / 5 timesteps per image)\n",
    "- $w_i$ = 1 for all images\n",
    "- $\\beta$ = 1 (I guess?)\n",
    "\n",
    "Assumptions:\n",
    "1. The blur kernel $H$ is uniform across all instances of $Y$\n",
    "2. The decimatimation kernel $D$ is uniform across all instances of $Y$\n",
    "3. The $w_i$ is uniform across all instances of $Y$, just a confidence of 1.0 (\"full confidence\")\n",
    "\n",
    "---\n",
    "\n",
    "#### To do:\n",
    "\n",
    "- [x] Generate 6 dataset subsets, containing timesteps each (i.e., 30 time steps / 5 time steps per file)\n",
    "- [x] Generate one point spread function per dataset subset, for analysis of different\n",
    "- [x] Generate one point spread function for the whole dataset\n",
    "- [x] Determine if there is a significant difference between a subset PSF and the full dataset PSF - this will be the blur kernel\n",
    "- [ ] Determine if the PSF can be reduced (i.e., dont use the full PSF) - what effects might this have?\n",
    "- [x] Generate an IDFT of the full dataset, $L^2$ pixels\n",
    "- [x] Genetate an IDFT for each of the subsets, $M^2$ pixels\n",
    "- [x] Come up with a formula for populating the decimation matrix\n",
    "- [ ] Come up with a formula for populating the blur matrix \n",
    "- [ ] Determine if there is a formula for populating the product of the blur and decimation matrix, such that $H^TD^T = (HD)^T$\n",
    "- [ ] take the average of optimal betas across support 20 to 47, use that for all supports, plot the error between solved x and true x using the averaged beta, and compare that against another plot which is the \"optimal\" beta vs support\n",
    "- [ ] another thing to try, consider trimming the edge of solved x before taking norms and measuring RRMSE, as the convolution matrices are not padded and so convolution is essentially \"incomplete\" around the image edges. Probably only need to take say 10-15 pixels off all edges, since most of the PSF is noise aside from the center.\n",
    "- [ ] Another thing to try is to produce a set of Y images which are only representative of a single source, like a PSF. Then I could run the SOLE algorithm to determine what the effects of solving are against the curve of a point source, by taking a middle slice of the image (like I do when inspecting the actual PSF). Could examine what happens when using different supports and beta terms to see which lines up best with the true curve.\n",
    " - [ ] I need to modify the decimation matrix function to support more than just 4 neighbours (or 1/2 decimation).\n",
    " - [ ] Modify the project so it is more like a library, where I can call my own functions in my desired order. This would make repeating experiments easier than one flat notebook.\n",
    " - [ ] Implement the laplacian of gaussian function, so that I can have a dynamically sized laplacian instead of a fixed 3x3 (note: requires another config to specify laplacian support size, and a sigma value to determine curve spread, or are they roughly the same type of thing?)\n",
    " - [ ] Try running the algorithm with the averaged beta, using different sized LoGs and different PSF supports to see what happens re: rrmse\n",
    " - [ ] Figure out how I can get the sigma using a point of inflection of a large PSF (something like that?)\n",
    " - [ ] Implement assertion to confirm number of y samples and decimation factor follow critical formula from adaptive filtering approach paper\n",
    " - [ ] June 9 - Andrew suggests updating my decimation routine to be more dynamic, supporting powers of 2 or even arbitrary amounts, not sure how hard that is? He suggests experimenting with variable decimation and using a fixed optimal beta, fixed optimal sigma, and fixed PSF support (one of the higher end supports which seem stable, say 51? Could probably even just do the full PSF of 99x99 realistically\n",
    "\n",
    "---\n",
    "\n",
    "#### Notes:\n",
    "\n",
    "- If one takes the $\\beta S^T S$ portion of $A$ and multiplies it by the solved $X$;  assuming the result is not normalised, one can observe the contribution of beta-influenced laplacian sharpening towards $B$. It appears to float around 10% of the second portion of $A$ (the decimated PSF) multiplied by the solved $X$. Taking the sum of these two separate portions of $AX$ allow one to produce $B$. It appears that the laplacian sharpening portion of $A$ does meaningfully contribute to help find a reasonable solution to $X$, and doesnt appear to be dominating (i.e., drowning out) the decimated PSF portion of $A$.\n",
    "- Essentially, we dont want the beta-driven laplacian to dominate the sharpened PSF, otherwise you end up with effectively an overly smooth image $X$, and the technique becomes less useful / less impressive. We want to solve for $X$, such that beta helps us resolve a more accurate image $X$ which isnt just noise (a lack of structure, looks like static), and also doesnt look too smooth. If solving was less useful because we required large beta terms, then we might as well just average our set of $Y$ low-resolution images instead since its computationally cheaper?\n",
    " - Implement nifty gridder as a jupyter notebook - Andrew wants to see an image of true x, solved x, and nifty x\n",
    " - Andrew suggestions that in the future, a good method for analysing the effectiveness of this technique would be to compare the extractable sources and compare against a sky model (possibly Fred Dulwich has this?). It seems RRMSE is \"misleading\", in the sense that images produced by this technique arent so noisy, sources appear more gaussian-like and less like the dirty beam.\n",
    " - Furthermore, I should have a look at how this technique compares against the direct image model, versus an image produced by means of gridding (i.e., w-projection, nifty).\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58702d96-425b-4e12-8018-899e631892b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "# import cupy as cp\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import convolve2d\n",
    "from scipy.stats import norm\n",
    "from scipy.sparse import csr_matrix, bsr_matrix\n",
    "from scipy.sparse.linalg import spsolve\n",
    "from scipy.linalg import solve\n",
    "from skimage import data, io, color\n",
    "from skimage.transform import resize\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [10, 10]\n",
    "\n",
    "def show_image(image, title, flip_x_axis=False):\n",
    "    if flip_x_axis:\n",
    "        image = np.fliplr(image)\n",
    "    plt.imshow(image, cmap=plt.get_cmap(\"gray\"))\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "    \n",
    "def normalise(data):\n",
    "    return (data - np.min(data)) / (np.max(data) - np.min(data))\n",
    "\n",
    "def rrmse(observed, ideal, decimal=6):\n",
    "    return \"{:.{}f}\".format(np.sqrt((1 / observed.shape[0]**2) * np.sum((observed-ideal)**2) / np.sum(ideal**2)) * 100.0, decimal)\n",
    "\n",
    "def laplacian_of_gaussian(x, y, sigma):\n",
    "    p = (x**2.0 + y**2.0) / 2.0 * sigma**2.0\n",
    "    return -(1.0 / (np.pi * sigma**4.0)) * (1.0 - p) * np.exp(-p)\n",
    "\n",
    "# def decimation_matrix(l, m):\n",
    "#     d_matrix = np.zeros((m**2, l**2), dtype=np.float32)\n",
    "\n",
    "#     tile = np.repeat((1, 0, 1), (2, l - 2, 2)) # assuming taking 2 neighbours per dimension\n",
    "#     t_len = tile.shape[0]\n",
    "#     d = l // m\n",
    "#     r_offset = m**2 // 2\n",
    "#     c_offset = l**2 // 2\n",
    "\n",
    "#     for p in np.arange(l//4): # divide by 4 as 4 neighbours total\n",
    "#         p_offset = p * l\n",
    "#         for q in np.arange(m):\n",
    "#             d_matrix[q+ p_offset//2, q*d + p_offset*2 : q*d+t_len + p_offset*2] = tile # top-left quadrant\n",
    "#             d_matrix[q+r_offset + p_offset//2, q*d+c_offset + p_offset*2: q*d+t_len+c_offset + p_offset*2] = tile # bottom-right quadrant\n",
    "#     return d_matrix\n",
    "\n",
    "# Generates a sparse decimation matrix using the Block Sparse Row matrix format (https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.bsr_matrix.html#scipy-sparse-bsr-matrix)\n",
    "def decimation_matrix(original_dim, downsample_factor):\n",
    "    \n",
    "    if original_dim % downsample_factor != 0:\n",
    "        raise ValueError(f\"Downsample factor {downsample_factor} is not a valid factor of your matrix dimension {original_dim}.\")\n",
    "    if downsample_factor == original_dim:\n",
    "        raise ValueError(f\"Downsample factor {downsample_factor} cannot be the same as your matrix dimension {original_dim}.\")\n",
    "    if downsample_factor == 1: # effectively, no downsampling\n",
    "        return np.identity(original_dim**2)\n",
    "    # Otherwise assumed you want to downsample by a valid factor of original_dim...\n",
    "    \n",
    "    sampling_regions_per_dim = original_dim // downsample_factor\n",
    "    samples_per_region_dim = downsample_factor\n",
    "    # print(f\"Sampling regions per dimension: {sampling_regions_per_dim}\")\n",
    "    # print(f\"Samples per dimension: {samples_per_region_dim}\")\n",
    "    non_zero_entries = sampling_regions_per_dim**2 * samples_per_region_dim**2\n",
    "    # print(f\"Non-zero entries: {non_zero_entries}\")\n",
    "    \n",
    "    rows = np.zeros(non_zero_entries, dtype=np.uintc)   # stores row indices for non-zero compressed sparse matrix entries\n",
    "    cols = np.zeros(non_zero_entries, dtype=np.uintc)   # stores col indices for non-zero compressed sparse matrix entries\n",
    "    vals = np.ones(non_zero_entries, dtype=np.float32)  # stores element value at [row, col] for non-zero entries\n",
    "    \n",
    "    # Generates linear x,y index strides for downsampling\n",
    "    sample_stride_1D = np.arange(0, original_dim, downsample_factor)\n",
    "    # print(sample_stride_1D)\n",
    "    mesh = np.array(np.meshgrid(sample_stride_1D, sample_stride_1D))\n",
    "    sample_strides_2D = mesh.T.reshape(-1, 2)\n",
    "  \n",
    "    neighbour_strides_1D = np.arange(samples_per_region_dim)\n",
    "    neighbour_mesh = np.array(np.meshgrid(neighbour_strides_1D, neighbour_strides_1D))\n",
    "\n",
    "    for index in np.arange(sample_strides_2D.shape[0]):\n",
    "        neighbour_coords = neighbour_mesh.T.reshape(-1, 2) + sample_strides_2D[index] # generates (row, col) index pair for the nxn neighbours of each sampling point in sample_strides_2D\n",
    "        neighbour_coords[:, 0] *= original_dim # scale y coord by high-resolution image dim to enable row striding (due to column-vector matrix flattening)\n",
    "        neighbour_coords = np.sum(neighbour_coords, axis=1) # combine x and y coord into single array index\n",
    "        rows[index * neighbour_coords.shape[0] : (index + 1) * neighbour_coords.shape[0]] = index\n",
    "        cols[index * neighbour_coords.shape[0] : (index + 1) * neighbour_coords.shape[0]] = neighbour_coords\n",
    "        \n",
    "    return bsr_matrix((vals, (rows, cols)))\n",
    "\n",
    "# Generates a sparse convolution matrix using the Block Sparse Row matrix format (https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.bsr_matrix.html#scipy-sparse-bsr-matrix)\n",
    "def convolution_matrix(l, kernel):\n",
    "    kernel_full_supp = kernel.shape[0]\n",
    "    # print(f\"Kernel full supp: {kernel_full_supp}\")\n",
    "    kernel_half_supp = (kernel_full_supp - 1) // 2\n",
    "    # print(f\"Kernel half supp: {kernel_half_supp}\")\n",
    "    non_zero_entries = kernel_full_supp**2 * l**2\n",
    "    # print(f\"Total non-zero entries: {non_zero_entries}\")\n",
    "    # print(f\"Or in MB: {non_zero_entries / 10**6}\")\n",
    "    rows = np.arange(non_zero_entries, dtype=np.uintc)\n",
    "    cols = np.arange(non_zero_entries, dtype=np.intc)\n",
    "    vals = np.arange(non_zero_entries, dtype=np.float32)\n",
    "\n",
    "    print(f\"Rows: {rows.size * rows.itemsize / 10**6} MB\")\n",
    "    print(f\"Cols: {cols.size * cols.itemsize / 10**6} MB\")\n",
    "    print(f\"Vals: {vals.size * vals.itemsize / 10**6} MB\")\n",
    "    \n",
    "#     print(rows)\n",
    "#     print(cols)\n",
    "#     print(vals)\n",
    "    \n",
    "#     # for each flattened \"convolution\" row in matrix\n",
    "#     for m_row in np.arange(l**2):\n",
    "#         # print(f\"On m_row {m_row}\")\n",
    "#         # map flattened m_row to 2d row/col  \n",
    "#         row, col = (m_row // l, m_row % l)\n",
    "#         # map kernel to neighbouring indices of row/col, from centre of kernel\n",
    "#         neighbour_rows =  np.arange(-(kernel_half_supp), kernel_half_supp+1) + row\n",
    "#         neighbour_cols = np.arange(-(kernel_half_supp), kernel_half_supp+1) + col\n",
    "#         mesh = np.array(np.meshgrid(neighbour_rows, neighbour_cols))\n",
    "#         neighbour_indices = mesh.T.reshape(-1, 2)\n",
    "#         # zero out in kernel, any neighbours which have negative index or greater than l\n",
    "#         bad_rows = np.concatenate((np.where(neighbour_indices[:, 0] < 0)[0], np.where(neighbour_indices[:, 0] >= l)[0]))\n",
    "#         bad_cols = np.concatenate((np.where(neighbour_indices[:, 1] < 0)[0], np.where(neighbour_indices[:, 1] >= l)[0]))\n",
    "#         # get a set of distinct neighbours to be zeroed out (in case of duplicate entries where a bad row is also a bad column)\n",
    "#         bad_neighbours = np.unique(np.concatenate((bad_rows, bad_cols)))\n",
    "#         # flatten 2d kernel and zero out invalid entries\n",
    "#         kernel = kernel.flatten()\n",
    "#         kernel[bad_neighbours] = 0.0\n",
    "#         # determine where to copy over row/col/kernel entries\n",
    "#         start = m_row * kernel_full_supp**2\n",
    "#         end = (m_row + 1) * kernel_full_supp**2\n",
    "#         col_strides = np.repeat((np.arange(-kernel_half_supp, kernel_half_supp + 1)), (np.repeat(kernel_full_supp, kernel_full_supp))) * (l - kernel_full_supp)\n",
    "#         # populating row, strided col indices and flattened kernel slices\n",
    "#         rows[start:end] = m_row\n",
    "#         cols[start:end] = (np.arange(m_row, m_row + kernel_full_supp**2) - kernel_full_supp**2 // 2 + col_strides)\n",
    "#         vals[start:end] = kernel\n",
    "\n",
    "#     # strip away any entries in rows/cols/vals where cols < 0, cols >= l**2, or where val is zero\n",
    "#     out_of_bounds_or_zero = np.concatenate((np.where(cols < 0), np.where(cols >= l**2), np.where(vals == 0.0)), axis=None)\n",
    "#     rows = np.delete(rows, out_of_bounds_or_zero)\n",
    "#     cols = np.delete(cols, out_of_bounds_or_zero)\n",
    "#     vals = np.delete(vals, out_of_bounds_or_zero)\n",
    "    \n",
    "    print(\"BUILDING UP BSR MATRIX RIGHT NOW...\")\n",
    "    pass\n",
    "    # return csr_matrix((vals, (rows, cols)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b98346-6318-477d-8555-44909edbde9c",
   "metadata": {},
   "source": [
    "#### Configuration and data set up..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a26282b-1625-4678-bfde-510ae1a36b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l^2:                      160000\n",
      "(psf_support - 2) * m**2: 970000\n",
      "n * m**2:                 300000\n",
      "Super-resolution imaging technically possible, continuing...\n",
      "\n",
      "D matrix compressed sparse: 1.320004 MB\n"
     ]
    }
   ],
   "source": [
    "timesteps = 30 # total timesteps\n",
    "timesteps_per_y = 1\n",
    "l = 400\n",
    "m = 100\n",
    "n = timesteps // timesteps_per_y\n",
    "w = np.ones(n)\n",
    "β = 8.0\n",
    "\n",
    "# False = use static 3x3 laplacian kernel\n",
    "# True = use customisable laplacian of gaussian kernel\n",
    "use_laplacian_of_gaussian = True\n",
    "\n",
    "# all time steps direct image\n",
    "# filename = \"../data/direct_image_ts_0_29.bin\"\n",
    "# filename = \"../data/direct_image_ts_0_29_800x800.bin\"\n",
    "filename = \"../datasets/gleam_small/images/direct_image_ts_0_29_400x400.bin\"\n",
    "x_true = np.fromfile(filename, dtype=np.float32)\n",
    "x_true = np.reshape(x_true, (l, l))\n",
    "# x_true = resize(x_true.reshape(800, 800), (l, l), anti_aliasing=False, order=1)\n",
    "x_true = normalise(x_true)\n",
    "# show_image(x_true, \"True X\")\n",
    "\n",
    "# x_orig = np.fromfile(\"../data/direct_image_ts_0_29.bin\", dtype=np.float32)\n",
    "# show_image(normalise(x_orig.reshape(l, l)), \"Original True X (100^2)\")\n",
    "\n",
    "# all time steps direct psf\n",
    "# filename = \"../data/direct_psf_ts_0_29.bin\"\n",
    "# filename = \"../data/direct_psf_ts_0_29_800x800.bin\"\n",
    "# x_psf = np.fromfile(filename, dtype=np.float32)\n",
    "# x_psf = resize(x_psf.reshape(800, 800), (l, l), anti_aliasing=False, order=1)\n",
    "# x_psf = x_psf.reshape(l, l)[1:, 1:] # trim row 0 and all column 0 (to ensure odd dimensions with the peak in the center)\n",
    "# x_psf /= np.max(x_psf)\n",
    "# show_image(x_psf, \"PSF (800^2 averaged to 100^2)\")\n",
    "\n",
    "# filename = \"../data/direct_psf_ts_0_29_800x800.bin\"\n",
    "filename = \"../datasets/gleam_small/images/direct_psf_ts_0_29_400x400.bin\"\n",
    "x_psf = np.fromfile(filename, dtype=np.float32).reshape(400, 400)[1:, 1:]\n",
    "# show_image(x_psf, \"Untrimmed PSF\")\n",
    "x_psf = resize(x_psf, (l-1, l-1), anti_aliasing=False, order=1)\n",
    "x_psf = np.pad(x_psf, ((1, 0), (1, 0))) # pad with new 0th row/col to ensure trimming from centre\n",
    "# show_image(x_psf, \"Untrimmed PSF - RESIZED\")\n",
    "# plt.plot(x_psf[x_psf.shape[0]//2])\n",
    "# plt.show()\n",
    "\n",
    "trim_half_len = 50\n",
    "psf_min = l//2 - (trim_half_len - 1)\n",
    "psf_max = l//2 + trim_half_len\n",
    "psf_support = psf_max - psf_min\n",
    "# print(psf_support)\n",
    "# x_psf = x_psf.reshape(l, l)[psf_min:psf_max, psf_min:psf_max]\n",
    "x_psf_trimmed = x_psf.copy()[psf_min:psf_max, psf_min:psf_max]\n",
    "x_psf_trimmed /= np.sum(x_psf_trimmed)\n",
    "# print(np.sum(x_psf_trimmed))\n",
    "# plt.plot(x_psf[x_psf.shape[0]//2])\n",
    "# plt.show()\n",
    "\n",
    "# print(f\"PSF Shape: {x_psf_trimmed.shape[0]}\")\n",
    "# show_image(x_psf_trimmed, \"PSF\")\n",
    "\n",
    "print(f\"l^2:                      {l**2}\")\n",
    "print(f\"(psf_support - 2) * m**2: {(psf_support - 2) * m**2}\")\n",
    "print(f\"n * m**2:                 {n * m**2}\")\n",
    "\n",
    "# Determine whether we have the right criteria to perform super-resolution imaging to obtain X\n",
    "if l**2 <= np.minimum((psf_support - 2) * m**2, n * m**2):\n",
    "    print(\"Super-resolution imaging technically possible, continuing...\\n\")\n",
    "else:\n",
    "    raise ValueError(\"High-resolution image not possible to reproduce from your set up, review your configuration against this if statement conditional.\\n\")\n",
    "\n",
    "# Storing all low-res images as layered stack\n",
    "y = np.zeros((n, m, m))\n",
    "\n",
    "# batched time steps direct images\n",
    "for i in np.arange(n):\n",
    "    timestep_range_start = i * timesteps_per_y\n",
    "    timestep_range_end = timestep_range_start + timesteps_per_y\n",
    "    filename = f\"../datasets/gleam_small/images/direct_image_ts_{timestep_range_start}_{timestep_range_end - 1}_{m}x{m}.bin\"\n",
    "    y[i] = np.fromfile(filename, dtype=np.float32).reshape(m, m)\n",
    "    y[i] = normalise(y[i])\n",
    "    # show_image(y[i], f\"$Y {i}$\")\n",
    "    \n",
    "# batched time steps point spread functions\n",
    "# for i in np.arange(N):\n",
    "#     start = i * timesteps_per_y\n",
    "#     end = start + timesteps_per_y - 1\n",
    "#     filename = \"../data/direct_psf_ts_%d_%d.bin\" % (start, end)\n",
    "#     Y_i_psf = np.fromfile(filename, dtype=np.float32)\n",
    "#     Y_i_psf = Y_i_psf.reshape(L, L)\n",
    "#     # show_image(Y_i_psf, \"$Y_{%d}$ PSF\" % i)\n",
    "\n",
    "# Decimation matrix\n",
    "d = decimation_matrix(l, 4)\n",
    "# d_numpy_size_bytes = d.size * d.itemsize\n",
    "# print(f\"D matrix uncompressed sparse: {d_numpy_size_bytes / 10**6} MB\")\n",
    "# d = bsr_matrix(d)\n",
    "d_csr_matrix_bytes = d.data.nbytes + d.indptr.nbytes + d.indices.nbytes\n",
    "print(f\"D matrix compressed sparse: {d_csr_matrix_bytes / (10**6)} MB\")\n",
    "# print(f\"D matrix reduced by {100 - (d_csr_matrix_bytes / d_numpy_size_bytes * 100)}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7009a576-6ec3-4a79-9a02-8273c137ebb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 6272.64 MB\n",
      "Cols: 6272.64 MB\n",
      "Vals: 6272.64 MB\n",
      "BUILDING UP BSR MATRIX RIGHT NOW...\n"
     ]
    }
   ],
   "source": [
    "# Blur matrix (psf)\n",
    "convolution_matrix(l, x_psf_trimmed)\n",
    "# print(\"H returned...\")\n",
    "# h_numpy_size_bytes = h.size * h.itemsize\n",
    "# print(f\"H matrix uncompressed sparse: {h_numpy_size_bytes / 10**6} MB\")\n",
    "# h = bsr_matrix(h)\n",
    "# h_csr_matrix_bytes = h.data.nbytes + h.indptr.nbytes + h.indices.nbytes\n",
    "# print(f\"H matrix compressed sparse: {h_csr_matrix_bytes / (10**9)} GB\")\n",
    "# print(f\"H matrix reduced by {100 - (h_csr_matrix_bytes / h_numpy_size_bytes * 100)}%\\n\")\n",
    "# show_image(h.todense(), \"H\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd6a15f-0815-4697-a031-391d2eba1650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sharpening matrix (laplacian)\n",
    "if use_laplacian_of_gaussian:\n",
    "    psf_central_support = 60 # 15 # num pixels around centre of PSF which represent approx 1/3 of the curve, maybe a bit bigger for padding\n",
    "    downsampling = np.linspace(-(psf_central_support-1)//2, (psf_central_support-1)//2, num=psf_central_support)\n",
    "    lap_of_gauss = np.zeros((downsampling.shape[0], downsampling.shape[0]), np.float32)\n",
    "    lap_gauss_σ = 2.205128205 # need to play around with this to get similar structure to true laplacian 3x3\n",
    "    for i in np.arange(downsampling.shape[0]):\n",
    "        for j in np.arange(downsampling.shape[0]):\n",
    "            lap_of_gauss[i][j] = -laplacian_of_gaussian(downsampling[i], downsampling[j], lap_gauss_σ)\n",
    "\n",
    "    lap_of_gauss /= np.max(lap_of_gauss)\n",
    "\n",
    "    # plt.plot(lap_of_gauss[lap_of_gauss.shape[0]//2])\n",
    "    # plt.show()\n",
    "\n",
    "    s = convolution_matrix(l, lap_of_gauss)\n",
    "    β *= 4.0\n",
    "    # print(f\"Laplacian Shape: {lap_of_gauss.shape[0]}\")\n",
    "else:\n",
    "    laplacian = np.array([[0, -1,  0], [-1,  4, -1], [0, -1,  0]], dtype=np.float32)\n",
    "    s = convolution_matrix(l, laplacian)\n",
    "\n",
    "s_numpy_size_bytes = s.size * s.itemsize\n",
    "print(f\"S matrix uncompressed sparse: {s_numpy_size_bytes / 10**6} MB\")\n",
    "s = bsr_matrix(s)\n",
    "s_csr_matrix_bytes = s.data.nbytes + s.indptr.nbytes + s.indices.nbytes\n",
    "print(f\"S matrix compressed sparse: {s_csr_matrix_bytes / (10**6)} MB\")\n",
    "print(f\"S matrix reduced by {100 - (s_csr_matrix_bytes / s_numpy_size_bytes * 100)}%\\n\")\n",
    "    \n",
    "dh = d @ h\n",
    "    \n",
    "# Notes to self: seems like lap_gauss_σ sits around 1.41 for this example, any higher or lower results in worse RRMSE. Changing the psf_central_support makes no difference, and taking more samples (i.e. num)\n",
    "# just means that lap_gauss_σ is multiplied by the factor of difference (i.e., double the num samples needs double the lap_gauss_σ). Also found that beta is best multiplied by 4.0, not 16 (the square).\n",
    "# Not sure what to do from here... seems like the laplacian of gaussian doesnt really serve any beneficial purpose..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f5877a-f42d-4bbe-9815-09fba1baae3f",
   "metadata": {},
   "source": [
    "#### Setting up the right hand side of the equation $AX = B$, where $B = \\sum\\limits_{i=1}^N (w_iH^TD^TY_i)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70b5b39-97ff-454d-a18c-be18ca528ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.zeros(l**2, dtype=np.float32)\n",
    "\n",
    "for i in np.arange(n):\n",
    "    b += w[i] * bsr_matrix.transpose(dh) @ y[i].flatten()\n",
    "    \n",
    "# show_image(normalise(b.reshape(l, l)), \"B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1731d55-9b86-441d-8959-fb7125ec1a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "print((b.size * b.itemsize) / 10**6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42023895-557c-4c97-a10e-bf1a2b43d09a",
   "metadata": {},
   "source": [
    "#### Setting up the right hand side of the equation $AX=B$, where $A = \\begin{bmatrix}\n",
    "  \\beta S^T S + (\\sum\\limits_{i=1}^N w_i) H^TD^TDH\n",
    "\\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cad2eee-2322-4931-825f-0efa8ca8c419",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# lhs = β * np.matmul(s.T, s)\n",
    "# rhs = (h.T @ d.T @ d @ h) * np.sum(w)\n",
    "a = (β * s @ bsr_matrix.transpose(s)) + (bsr_matrix.transpose(dh) @ dh * np.sum(w))\n",
    "\n",
    "# a = bsr_matrix.toarray(a)\n",
    "# show_image(a.reshape(l**2, l**2), \"A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7776bd4-4c3f-4f22-aa51-84177a5daa5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(a))\n",
    "print((a.data.nbytes + a.indptr.nbytes + a.indices.nbytes) / 10**6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f646df-d788-465a-b6b6-f0a308d22e9a",
   "metadata": {},
   "source": [
    "#### Now solve for $X$..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c87506-72dd-42e6-8e3e-c83c821d9a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Solving via CPU and numpy...\n",
    "# x = np.linalg.solve(a, b)\n",
    "# x = spsolve(a, b) # around 2 mins 10 seconds for this routine\n",
    "x = solve(a.todense(), b) # wow, about 5 seconds...\n",
    "x = x.reshape(l, l)\n",
    "# x = normalise(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1855c1fa-46d8-467a-9405-a59a7fb648b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Solving via GPU and cupy\n",
    "# a_gpu = cp.asarray(a)\n",
    "# b_gpu = cp.asarray(b)\n",
    "# solution = cp.linalg.solve(a_gpu, b_gpu)\n",
    "# x = cp.asnumpy(solution)\n",
    "# x = x.reshape(100, 100)\n",
    "\n",
    "# # Dealloc cuda mem\n",
    "# mempool = cp.get_default_memory_pool()\n",
    "# mempool.free_all_blocks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6764572-2498-4772-8051-feb390d94eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# b_true = np.matmul(normalise(a), x_true.flatten())\n",
    "# x_accurate = np.linalg.solve(a, b_true)\n",
    "# x_accurate = x_accurate.reshape(100, 100)\n",
    "\n",
    "# show_image(normalise(x), \"Solved X\")\n",
    "# print(f\"RRMSE: B and True B -> {rrmse(normalise(b.reshape(l, l)), normalise(b_true.reshape(l, l)))}\")\n",
    "# print(f\"L2 between B and True X: {np.sqrt(np.sum((normalise(b.reshape(l, l))-normalise(x))**2))}\")\n",
    "# show_image(normalise(b.reshape(l, l)), \"B\")\n",
    "# show_image(normalise(b_true.reshape(l, l)), \"True B\")\n",
    "\n",
    "# print(f\"RRMSE: X and Accurate X -> {rrmse(normalise(x), normalise(x_accurate))}\")\n",
    "# print(f\"RRMSE: Accurate X and True X -> {rrmse(normalise(x_accurate), normalise(x_true))}\")\n",
    "print(f\"Beta: {β}\")\n",
    "print(f\"RRMSE: Solved X and True X -> {rrmse(normalise(x), normalise(x_true))}\")\n",
    "print(f\"True X peak location: {np.unravel_index(np.argmax(x_true, axis=None), x_true.shape)}\")\n",
    "print(f\"Solved X peak location: {np.unravel_index(np.argmax(x, axis=None), x.shape)}\")\n",
    "\n",
    "# plt.plot(x_true[93, :], label=\"True X\")\n",
    "# plt.plot(x[93, :], label=\"Solved X\")\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# Multiply A left hand side (beta and sharpening matrices) by X\n",
    "# a_laplacian = lhs # np.matmul(lhs, x.flatten()).reshape(l, l)\n",
    "# show_image((a_laplacian), \"A-laplacian\", flip_x_axis=True)\n",
    "# a_lap_l1 = np.max(np.sum(np.absolute(a_laplacian), axis=0))\n",
    "# a_lap_linf = np.max(np.sum(np.absolute(a_laplacian), axis=1))\n",
    "# print(f\"A_laplacian l1 norm: {a_lap_l1}\")\n",
    "# print(f\"A_laplacian linf norm: {a_lap_linf}\")\n",
    "\n",
    "# a_deci_blur = rhs # np.matmul(rhs, x.flatten()).reshape(l, l)\n",
    "# show_image((a_deci_blur), \"A-deci-blur\", flip_x_axis=True)\n",
    "# a_dec_l1 = np.max(np.sum(np.absolute(a_deci_blur), axis=0))\n",
    "# a_dec_linf = np.max(np.sum(np.absolute(a_deci_blur), axis=1))\n",
    "# print(f\"A_deci_blur l1 norm: {a_dec_l1}\")\n",
    "# print(f\"A_deci_blur linf norm: {a_dec_linf}\")\n",
    "\n",
    "# print(f\"A_deci_blur / A_laplacian l1 ratio: {a_dec_l1 / a_lap_l1}\")\n",
    "# print(f\"A_deci_blur / A_laplacian linf ratio: {a_dec_linf / a_lap_linf}\")\n",
    "\n",
    "show_image(normalise(x), \"Solved X\", flip_x_axis=True)\n",
    "# show_image(normalise(x_accurate[10:l-11, 10:l-11]), \"Solved X using True X (IDFT)\")\n",
    "# show_image(normalise(x_accurate), \"Solved X using True X (IDFT)\")\n",
    "show_image(normalise(x_true), \"True X\", flip_x_axis=True)\n",
    "# show_image(normalise(a_laplacian + a_deci_blur), \"A parts summed to B\", flip_x_axis=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50af05e-2456-4ddc-8de8-7f487ae41b3f",
   "metadata": {},
   "source": [
    "### Experimenting with different high/low resolution image sizes with variable number of low-resolution images\n",
    "\n",
    "- Dataset: gleam small\n",
    "- High-resolution image size: generated at 800x800, downsampled to 100x100 for solving in reasonable time\n",
    "- PSF support: 51x51\n",
    "\n",
    "| L   | M  | N  | Beta        | Beta Scaled | Laplacian or LoG | LoG Sigma   | RRMSE (%) | \n",
    "| -   | -  | -  | -           | -           | -                | -           | -         | \n",
    "| 100 | 50 | 6  | 0.217189985 | N/A         | Laplacian        | N/A         | 1.144697  |\n",
    "| 100 | 50 | 6  | 0.217189985 | 0.86875994  | LoG              | 2.205128205 | 0.763645  |\n",
    "| -   | -  | -  | -           | -           | -                | -           | -         | \n",
    "| 100 | 25 | 30 | 0.217189985 | N/A         | Laplacian        | N/A         | 2.763222  |\n",
    "| 100 | 25 | 30 | 0.217189985 | 0.86875994  | LoG              | 2.205128205 | 2.530623  |\n",
    "| 100 | 25 | 30 | 0.4         | 1.6         | LoG              | 2.205128205 | 2.221223  |\n",
    "| 100 | 25 | 30 | 0.6         | 2.4         | LoG              | 2.205128205 | 1.990213  |\n",
    "| 100 | 25 | 30 | 1.0         | 4.0         | LoG              | 2.205128205 | 1.636430  |\n",
    "| 100 | 25 | 30 | 1.25        | 5.0         | LoG              | 2.205128205 | 1.555625  |\n",
    "| 100 | 25 | 30 | 1.5         | 6.0         | LoG              | 2.205128205 | 1.524417  |\n",
    "| 100 | 25 | 30 | 2.0         | 8.0         | LoG              | 2.205128205 | 1.460980  |\n",
    "| 100 | 25 | 30 | 3.0         | 12.0        | LoG              | 2.205128205 | 1.336314  |\n",
    "| 100 | 25 | 30 | 4.0         | 16.0        | LoG              | 2.205128205 | 1.229604  |\n",
    "| 100 | 25 | 30 | 5.0         | 20.0        | LoG              | 2.205128205 | 1.194185  |\n",
    "| 100 | 25 | 30 | 8.0         | 32.0        | LoG              | 2.205128205 | 1.086680  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1a93a2-8a21-4121-938a-9c9c45e39b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.tofile(f\"../datasets/gleam_small/images/solved_image_{x.shape[0]}x{x.shape[1]}.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84dc7797-9340-4dd0-a5ce-3463bb2de095",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
